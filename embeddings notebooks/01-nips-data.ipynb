{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZX9YF6v4p8B",
        "outputId": "f65d9564-d873-47d3-8cf3-c7e56e079cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ],
      "source": [
        "! pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab as plt\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import fitz\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "GbAgtlul4vOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# works for 2017-2000-1987\n",
        "year = 1987\n",
        "base_url = \"https://papers.neurips.cc\"\n",
        "book_url = f\"https://papers.neurips.cc/paper_files/paper/{year}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "resp = requests.get(book_url, headers=headers)\n",
        "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "papers = []\n",
        "for li in soup.select(\"ul.paper-list > li\"):\n",
        "    a = li.find(\"a\", attrs={\"title\": \"paper title\"})\n",
        "    i = li.find(\"i\")\n",
        "\n",
        "    if a and i:\n",
        "        href = a[\"href\"]\n",
        "        title = a.text.strip()\n",
        "        authors = i.text.strip()\n",
        "\n",
        "        match = re.search(r\"hash/([a-f0-9]{32})\", href)\n",
        "        if match:\n",
        "            hash_id = match.group(1)\n",
        "            papers.append({\n",
        "                \"hash_id\": hash_id,\n",
        "                'year' : year\n",
        "            })\n",
        "\n",
        "print(f\"Extracted {len(papers)} papers\")\n",
        "print(\"Sanity check:\", papers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzoSdJa42da",
        "outputId": "55a8daec-ea96-424a-e5bd-cd5be2e2653d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 90 papers\n",
            "Sanity check: {'hash_id': '03004620ea802b9118dd44d69f07af56', 'year': 1987}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year = 1987\n",
        "hash_id = '03004620ea802b9118dd44d69f07af56'\n",
        "url = f\"https://papers.neurips.cc/paper_files/paper/{year}/hash/{hash_id}-Abstract.html\"\n",
        "base_url = \"https://papers.neurips.cc\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "resp = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "title_meta = soup.find(\"meta\", attrs={\"name\": \"citation_title\"})\n",
        "title = title_meta[\"content\"] if title_meta else \"N/A\"\n",
        "\n",
        "authors = \"N/A\"\n",
        "for h4 in soup.find_all(\"h4\"):\n",
        "    if \"Authors\" in h4.text:\n",
        "        p_tag = h4.find_next_sibling(\"p\")\n",
        "        if p_tag:\n",
        "            i_tag = p_tag.find(\"i\")\n",
        "            if i_tag:\n",
        "                authors = i_tag.get_text(strip=True)\n",
        "        break\n",
        "\n",
        "pdf_meta = soup.find(\"meta\", attrs={\"name\": \"citation_pdf_url\"})\n",
        "pdf_url = pdf_meta[\"content\"] if pdf_meta else \"N/A\"\n",
        "\n",
        "abstract = \"N/A\"\n",
        "for h4 in soup.find_all(\"h4\"):\n",
        "    if \"Abstract\" in h4.text:\n",
        "        p = h4.find_next_sibling(\"p\")\n",
        "        if p:\n",
        "            abstract = p.text.strip()\n",
        "        break\n",
        "\n",
        "bib_button = soup.find(\"a\", string=\"Bibtex\")\n",
        "bib_url = base_url + bib_button[\"href\"] if bib_button else \"N/A\"\n",
        "\n",
        "pdf_url = f\"{pdf_url}\"\n",
        "\n",
        "print(\"Title:\", title)\n",
        "print(\"Authors:\", authors)\n",
        "print(\"PDF URL:\", pdf_url)\n",
        "print(\"BibTeX URL:\", bib_url)\n",
        "print(\"Abstract:\", abstract)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fufQaC-b46bN",
        "outputId": "78f4acba-d05e-4074-8221-1fa0b67b6d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Synchronization in Neural Nets\n",
            "Authors: Jacques J. Vidal, John Haggerty\n",
            "PDF URL: https://proceedings.neurips.cc/paper_files/paper/1987/file/03004620ea802b9118dd44d69f07af56-Paper.pdf\n",
            "BibTeX URL: https://papers.neurips.cc/paper_files/paper/1987/file/03004620ea802b9118dd44d69f07af56-Bibtex.bib\n",
            "Abstract: The  paper  presents  an  artificial  neural  network  concept  (the  Synchronizable Oscillator Networks)  where the instants of individual  firings  in  the  form  of  point  processes  constitute  the  only  form  of  information  transmitted  between  joining  neurons.  This  type  of  communication contrasts with  that which  is  assumed  in most  other  models  which  typically  are  continuous  or  discrete  value-passing  networks.  Limiting the messages received  by each processing unit to  time  markers that signal  the firing  of other units  presents  significant  implemen tation advantages. \n",
            "In  our  model,  neurons  fire  spontaneously  and  regularly  in  the  absence of perturbation.  When interaction is  present,  the scheduled  firings  are  advanced  or  delayed  by  the firing  of neighboring  neurons.  Networks  of  such  neurons  become  global  oscillators  which  exhibit  multiple  synchronizing  attractors.  From  arbitrary  initial  states,  energy  minimization  learning  procedures  can  make  the  network  converge  to  oscillatory  modes  that  satisfy  multi-dimensional  constraints  Such  networks  can  directly  represent  routing  and  scheduling problems  that conSist of ordering sequences of events.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "years = [2017,2016,2015,2014,2013,2012,2011,2010,2009,2008,2007,2006,2005,2004,2003,2002,2001,2000,1999,1998,1997,1996,1995,1994,1993,1992,1991,1990,1989,1988,1987]\n",
        "base_url = \"https://papers.neurips.cc\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "papers = []\n",
        "\n",
        "for year in years:\n",
        "    book_url = f\"{base_url}/paper_files/paper/{year}\"\n",
        "    resp = requests.get(book_url, headers=headers)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    for li in soup.select(\"ul.paper-list > li\"):\n",
        "        a = li.find(\"a\", attrs={\"title\": \"paper title\"})\n",
        "        if a:\n",
        "            href = a[\"href\"]\n",
        "            match = re.search(r\"hash/([a-f0-9]{32})\", href)\n",
        "            if match:\n",
        "                hash_id = match.group(1)\n",
        "                papers.append({\n",
        "                    \"hash_id\": hash_id,\n",
        "                    \"year\": year\n",
        "                })\n",
        "\n",
        "print(f\"Extracted {len(papers)} papers\")\n",
        "print(\"Sanity check:\", papers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD4ZcK1p6GFR",
        "outputId": "5cc18a0f-ef90-4b2c-b1b1-0de7a5ec4a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 7243 papers\n",
            "Sanity check: {'hash_id': '0060ef47b12160b9198302ebdb144dcf', 'year': 2017}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC2jR_y76hib",
        "outputId": "38978a21-7b03-4be4-9f0b-fe4c42e8722b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "paper_data = []\n",
        "counter = 0\n",
        "total = len(papers)\n",
        "\n",
        "for hashy in tqdm(papers, desc=\"Scraping NeurIPS Papers\"):\n",
        "    # counter += 1\n",
        "    # print(f\"[{counter}/{total}] Processing {hashy['year']} - {hashy['hash_id']}...\")\n",
        "\n",
        "    year = hashy['year']\n",
        "    hash_id = hashy['hash_id']\n",
        "    url = f\"https://papers.neurips.cc/paper_files/paper/{year}/hash/{hash_id}-Abstract.html\"\n",
        "    reviews_url = f\"https://papers.neurips.cc/paper_files/paper/{year}/file/{hash_id}-Reviews.html\"\n",
        "    reviews_url2 = f\"https://papers.neurips.cc/paper_files/paper/{year}/file/{hash_id}-Review.html\"\n",
        "    base_url = \"https://papers.neurips.cc\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        title_meta = soup.find(\"meta\", attrs={\"name\": \"citation_title\"})\n",
        "        title = title_meta[\"content\"] if title_meta else \"N/A\"\n",
        "\n",
        "        authors = \"N/A\"\n",
        "        for h4 in soup.find_all(\"h4\"):\n",
        "            if \"Authors\" in h4.text:\n",
        "                p_tag = h4.find_next_sibling(\"p\")\n",
        "                if p_tag:\n",
        "                    i_tag = p_tag.find(\"i\")\n",
        "                    if i_tag:\n",
        "                        authors = i_tag.get_text(strip=True)\n",
        "                break\n",
        "\n",
        "        pdf_meta = soup.find(\"meta\", attrs={\"name\": \"citation_pdf_url\"})\n",
        "        pdf_url = pdf_meta[\"content\"] if pdf_meta else \"N/A\"\n",
        "\n",
        "        abstract = \"N/A\"\n",
        "        for h4 in soup.find_all(\"h4\"):\n",
        "            if \"Abstract\" in h4.text:\n",
        "                p = h4.find_next_sibling(\"p\")\n",
        "                if p:\n",
        "                    abstract = p.text.strip()\n",
        "                break\n",
        "\n",
        "        bib_button = soup.find(\"a\", string=\"Bibtex\")\n",
        "        bib_url = base_url + bib_button[\"href\"] if bib_button else \"N/A\"\n",
        "\n",
        "        if pdf_url == 'N/A':\n",
        "            continue\n",
        "\n",
        "        pdf_url = f\"{pdf_url}\"\n",
        "\n",
        "        paper_data.append({\n",
        "            \"year\": year,\n",
        "            \"hash_id\": hash_id,\n",
        "            \"title\": title,\n",
        "            \"authors\": authors,\n",
        "            \"abstract\": abstract,\n",
        "            \"pdf_url\": pdf_url,\n",
        "            \"bib_url\": bib_url,\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {hash_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "papers_df = pd.DataFrame(paper_data)\n",
        "papers_df.to_csv('1987_2017_neurIPS_papers.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbsKW_Ku6m_n",
        "outputId": "d5867531-6219-4abd-b43e-0fbe5e9becff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping NeurIPS Papers:  17%|█▋        | 1243/7243 [06:14<64:56:03, 38.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing fe40fb944ee700392ed51bfe84dd4e3d: HTTPSConnectionPool(host='papers.neurips.cc', port=443): Max retries exceeded with url: /paper_files/paper/2016/hash/fe40fb944ee700392ed51bfe84dd4e3d-Abstract.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x79ad4e170150>, 'Connection to papers.neurips.cc timed out. (connect timeout=None)'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping NeurIPS Papers: 100%|██████████| 7243/7243 [26:00<00:00,  4.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJ8atZ_P6tLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}