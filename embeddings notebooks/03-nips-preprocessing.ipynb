{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtteqTwrgJTt",
        "outputId": "369b72a4-eedc-4b95-df87-e45d84ab5a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n"
          ]
        }
      ],
      "source": [
        "! pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab as plt\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import fitz\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "EZ1rnZL9gPjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploads = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "vEyIIuqwgXJH",
        "outputId": "09158607-bf8c-4051-d321-aa6e518f970d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e856470-63f2-49b1-bb51-2e8f939aafaf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e856470-63f2-49b1-bb51-2e8f939aafaf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1987-2024-nips-papers.csv to 1987-2024-nips-papers (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('1987-2024-nips-papers.csv')"
      ],
      "metadata": {
        "id": "uLcfh6zjgYo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def extract_text_from_pdf_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "            tmp.write(response.content)\n",
        "            tmp_path = tmp.name\n",
        "\n",
        "        doc = fitz.open(tmp_path)\n",
        "        text = \"\".join(page.get_text() for page in doc)\n",
        "        doc.close()\n",
        "\n",
        "        os.remove(tmp_path)\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n"
      ],
      "metadata": {
        "id": "OZrUu6KLgap1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_texts_parallel(urls, max_workers=16):\n",
        "    texts = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(tqdm(executor.map(extract_text_from_pdf_url, urls), total=len(urls)))\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "SzKOjpFak9nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "all_texts = []\n",
        "\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch_urls = df['pdf_url'].iloc[i:i+batch_size]\n",
        "    print(f\"Processing batch {i} to {i+len(batch_urls)}\")\n",
        "\n",
        "    texts = extract_texts_parallel(batch_urls, max_workers=16)\n",
        "    all_texts.extend(texts)\n",
        "\n",
        "    pd.Series(all_texts).to_csv(\"partial_texts.csv\", index=False)\n",
        "\n",
        "df['pdf_text'] = all_texts\n",
        "df.to_csv('02-anika-shmanika.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CmpdHJYkjNV",
        "outputId": "7b7a9953-0044-48fd-9a27-585af8948c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 0 to 100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:21<00:00,  4.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 100 to 200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 200 to 300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 300 to 400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 400 to 500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.03it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 500 to 600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 600 to 700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 700 to 800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 800 to 900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 900 to 1000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.08it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1000 to 1100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:26<00:00,  3.75it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1100 to 1200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.41it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1200 to 1300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1300 to 1400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  8.25it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1400 to 1500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1500 to 1600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1600 to 1700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  8.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1700 to 1800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1800 to 1900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1900 to 2000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2000 to 2100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2100 to 2200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2200 to 2300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2300 to 2400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2400 to 2500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.38it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2500 to 2600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  8.20it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2600 to 2700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:38<00:00,  2.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2700 to 2800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2800 to 2900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 2900 to 3000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3000 to 3100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3100 to 3200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3200 to 3300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.75it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3300 to 3400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  9.01it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3400 to 3500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3500 to 3600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.92it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3600 to 3700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  8.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3700 to 3800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3800 to 3900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.69it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 3900 to 4000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:21<00:00,  4.68it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4000 to 4100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4100 to 4200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4200 to 4300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 11.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4300 to 4400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.29it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4400 to 4500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.22it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4500 to 4600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4600 to 4700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 11.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4700 to 4800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4800 to 4900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:22<00:00,  4.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 4900 to 5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:10<00:00,  9.81it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5000 to 5100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:10<00:00,  9.87it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5100 to 5200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5200 to 5300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:10<00:00,  9.98it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5300 to 5400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5400 to 5500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5500 to 5600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5600 to 5700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5700 to 5800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5800 to 5900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.60it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 5900 to 6000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6000 to 6100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.52it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6100 to 6200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6200 to 6300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6300 to 6400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  8.25it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6400 to 6500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6500 to 6600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6600 to 6700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6700 to 6800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.68it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6800 to 6900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.92it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 6900 to 7000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7000 to 7100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7100 to 7200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7200 to 7300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7300 to 7400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.10it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7400 to 7500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7500 to 7600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.45it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7600 to 7700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7700 to 7800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7800 to 7900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 7900 to 8000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8000 to 8100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8100 to 8200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8200 to 8300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8300 to 8400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8400 to 8500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8500 to 8600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8600 to 8700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8700 to 8800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.05it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8800 to 8900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:19<00:00,  5.12it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 8900 to 9000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:19<00:00,  5.03it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9000 to 9100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9100 to 9200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.95it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9200 to 9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9300 to 9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9400 to 9500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9500 to 9600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9600 to 9700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.06it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9700 to 9800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9800 to 9900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 9900 to 10000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10000 to 10100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10100 to 10200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10200 to 10300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.53it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10300 to 10400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10400 to 10500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10500 to 10600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:19<00:00,  5.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10600 to 10700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  5.96it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10700 to 10800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.60it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10800 to 10900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.75it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 10900 to 11000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  5.98it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11000 to 11100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.61it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11100 to 11200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11200 to 11300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11300 to 11400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11400 to 11500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11500 to 11600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.19it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11600 to 11700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.22it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11700 to 11800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11800 to 11900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.94it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 11900 to 12000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.01it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 12000 to 12100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:33<00:00,  3.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 12100 to 12200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 12200 to 12300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 12300 to 12400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:19<00:00,  5.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12400 to 12500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12500 to 12600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:29<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12600 to 12700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12700 to 12800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12800 to 12900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12900 to 13000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13000 to 13100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13100 to 13200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13200 to 13300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13300 to 13400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  5.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13400 to 13500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13500 to 13600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13600 to 13700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13700 to 13800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13800 to 13900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13900 to 14000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14000 to 14100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14100 to 14200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14200 to 14300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:19<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14300 to 14400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  5.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14400 to 14500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14500 to 14600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:20<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14600 to 14700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14700 to 14800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14800 to 14900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14900 to 15000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15000 to 15100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15100 to 15200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15200 to 15300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15300 to 15400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15400 to 15500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15500 to 15600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15600 to 15700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15700 to 15800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:22<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15800 to 15900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15900 to 16000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:20<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16000 to 16100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16100 to 16200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16200 to 16300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16300 to 16400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:45<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16400 to 16500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16500 to 16600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:18<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16600 to 16700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:26<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16700 to 16800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:20<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16800 to 16900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:25<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16900 to 17000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:33<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17000 to 17100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17100 to 17200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:32<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17200 to 17300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17300 to 17400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17400 to 17500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:40<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17500 to 17600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:29<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17600 to 17700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17700 to 17800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17800 to 17900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:25<00:00,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17900 to 18000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18000 to 18100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18100 to 18200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:25<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18200 to 18300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18300 to 18400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18400 to 18500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18500 to 18600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:31<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18600 to 18700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:27<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18700 to 18800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:27<00:00,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18800 to 18900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:22<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18900 to 19000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19000 to 19100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:54<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19100 to 19200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:21<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19200 to 19300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:27<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19300 to 19400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19400 to 19500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:31<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19500 to 19600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19600 to 19700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19700 to 19800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:31<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19800 to 19900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:36<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19900 to 20000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20000 to 20100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20100 to 20200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:32<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20200 to 20300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20300 to 20400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:31<00:00,  3.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20400 to 20500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20500 to 20600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:34<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20600 to 20700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20700 to 20800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:41<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20800 to 20900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20900 to 21000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21000 to 21100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:26<00:00,  3.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21100 to 21200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:42<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21200 to 21300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:50<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21300 to 21400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:32<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21400 to 21500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:46<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21500 to 21600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21600 to 21700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:40<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21700 to 21800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21800 to 21900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:39<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21900 to 22000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:49<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22000 to 22100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:42<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22100 to 22200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:33<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22200 to 22300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22300 to 22400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:26<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22400 to 22500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22500 to 22600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22600 to 22700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22700 to 22800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22800 to 22900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:28<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22900 to 23000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:31<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23000 to 23100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:31<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23100 to 23200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:29<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23200 to 23300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:39<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23300 to 23400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:55<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23400 to 23500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23500 to 23600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23600 to 23700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23700 to 23800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23800 to 23900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:33<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23900 to 24000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24000 to 24100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24100 to 24200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:32<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24200 to 24300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:38<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24300 to 24400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24400 to 24500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24500 to 24600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:32<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24600 to 24700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24700 to 24779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:26<00:00,  2.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['pdf_text'] = all_texts\n",
        "df.to_csv('02-anika-shmanika.csv')"
      ],
      "metadata": {
        "id": "HeGi3SDbkuMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "wAIR36KH7N7y",
        "outputId": "3675c1d3-7916-4ae6-a842-e2e8c57a6bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  year                           hash_id  \\\n",
              "0           0  2017  0060ef47b12160b9198302ebdb144dcf   \n",
              "1           1  2017  0070d23b06b1486a538c0eaa45dd167a   \n",
              "2           2  2017  00a03ec6533ca7f5c644d198d815329c   \n",
              "3           3  2017  013a006f03dbc5392effeb8f18fda755   \n",
              "4           4  2017  01894d6f048493d2cacde3c579c315a3   \n",
              "\n",
              "                                               title  \\\n",
              "0  Real Time Image Saliency for Black Box Classif...   \n",
              "1  Joint distribution optimal transportation for ...   \n",
              "2  Learning A Structured Optimal Bipartite Graph ...   \n",
              "3          Learning to Inpaint for Image Compression   \n",
              "4         Inverse Filtering for Hidden Markov Models   \n",
              "\n",
              "                                             authors  \\\n",
              "0                         Piotr Dabkowski, Yarin Gal   \n",
              "1  Nicolas Courty, Rémi Flamary, Amaury Habrard, ...   \n",
              "2  Feiping Nie, Xiaoqian Wang, Cheng Deng, Heng H...   \n",
              "3  Mohammad Haris Baig, Vladlen Koltun, Lorenzo T...   \n",
              "4  Robert Mattila, Cristian Rojas, Vikram Krishna...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  In this work we develop a fast saliency detect...   \n",
              "1  This paper deals with the unsupervised domain ...   \n",
              "2  Co-clustering methods have been widely applied...   \n",
              "3  We study the design of deep architectures for ...   \n",
              "4  This paper considers a number of related inver...   \n",
              "\n",
              "                                             pdf_url  \\\n",
              "0  https://proceedings.neurips.cc/paper_files/pap...   \n",
              "1  https://proceedings.neurips.cc/paper_files/pap...   \n",
              "2  https://proceedings.neurips.cc/paper_files/pap...   \n",
              "3  https://proceedings.neurips.cc/paper_files/pap...   \n",
              "4  https://proceedings.neurips.cc/paper_files/pap...   \n",
              "\n",
              "                                            pdf_text  \n",
              "0  Real Time Image Saliency for Black Box Classiﬁ...  \n",
              "1  Joint distribution optimal transportation for ...  \n",
              "2  Learning A Structured Optimal Bipartite Graph\\...  \n",
              "3  Learning to Inpaint for Image Compression\\nMoh...  \n",
              "4  Inverse Filtering for Hidden Markov Models\\nRo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0dd79378-7160-45b3-ac7f-7c7dab4546c2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>year</th>\n",
              "      <th>hash_id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>abstract</th>\n",
              "      <th>pdf_url</th>\n",
              "      <th>pdf_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>0060ef47b12160b9198302ebdb144dcf</td>\n",
              "      <td>Real Time Image Saliency for Black Box Classif...</td>\n",
              "      <td>Piotr Dabkowski, Yarin Gal</td>\n",
              "      <td>In this work we develop a fast saliency detect...</td>\n",
              "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
              "      <td>Real Time Image Saliency for Black Box Classiﬁ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0070d23b06b1486a538c0eaa45dd167a</td>\n",
              "      <td>Joint distribution optimal transportation for ...</td>\n",
              "      <td>Nicolas Courty, Rémi Flamary, Amaury Habrard, ...</td>\n",
              "      <td>This paper deals with the unsupervised domain ...</td>\n",
              "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
              "      <td>Joint distribution optimal transportation for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2017</td>\n",
              "      <td>00a03ec6533ca7f5c644d198d815329c</td>\n",
              "      <td>Learning A Structured Optimal Bipartite Graph ...</td>\n",
              "      <td>Feiping Nie, Xiaoqian Wang, Cheng Deng, Heng H...</td>\n",
              "      <td>Co-clustering methods have been widely applied...</td>\n",
              "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
              "      <td>Learning A Structured Optimal Bipartite Graph\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2017</td>\n",
              "      <td>013a006f03dbc5392effeb8f18fda755</td>\n",
              "      <td>Learning to Inpaint for Image Compression</td>\n",
              "      <td>Mohammad Haris Baig, Vladlen Koltun, Lorenzo T...</td>\n",
              "      <td>We study the design of deep architectures for ...</td>\n",
              "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
              "      <td>Learning to Inpaint for Image Compression\\nMoh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>01894d6f048493d2cacde3c579c315a3</td>\n",
              "      <td>Inverse Filtering for Hidden Markov Models</td>\n",
              "      <td>Robert Mattila, Cristian Rojas, Vikram Krishna...</td>\n",
              "      <td>This paper considers a number of related inver...</td>\n",
              "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
              "      <td>Inverse Filtering for Hidden Markov Models\\nRo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dd79378-7160-45b3-ac7f-7c7dab4546c2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0dd79378-7160-45b3-ac7f-7c7dab4546c2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0dd79378-7160-45b3-ac7f-7c7dab4546c2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e046a814-a9e8-474a-b816-06d5a85e3142\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e046a814-a9e8-474a-b816-06d5a85e3142')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e046a814-a9e8-474a-b816-06d5a85e3142 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24779,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7153,\n        \"min\": 0,\n        \"max\": 24778,\n        \"num_unique_values\": 24779,\n        \"samples\": [\n          16101,\n          4189,\n          6951\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2024,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          2020,\n          2023,\n          2013\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hash_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16467,\n        \"samples\": [\n          \"f1c1592588411002af340cbaedd6fc33\",\n          \"70b8505ac79e3e131756f793cd80eb8d\",\n          \"2f6a6317bada76b26a4f61bb70a7db59\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24779,\n        \"samples\": [\n          \"Matryoshka Representation Learning\",\n          \"Temporal dynamics of information content carried by neurons in the primary visual cortex\",\n          \"Stereopsis by a Neural Network Which Learns the Constraints\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23893,\n        \"samples\": [\n          \"Kai Liu, Haotong Qin, Yong Guo, Xin Yuan, Linghe Kong, Guihai Chen, Yulun Zhang\",\n          \"He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou\",\n          \"Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24666,\n        \"samples\": [\n          \"When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a significant impact on memory retention.  Behavioral experiments have shown a nonmonotonic relationship between spacing and retention:  short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals.  Appropriate spacing of study can double retention on educationally relevant time scales.  We introduce a Multiscale Context Model (MCM) that is able to predict the influence of a particular study schedule on retention for specific material.  MCMs prediction is based on empirical data characterizing forgetting of the material following a single study session.  MCM is a synthesis of two existing memory models (Staddon, Chelaru, & Higa, 2002; Raaijmakers, 2003).  On the surface, these  models are unrelated and incompatible, but we show they share a core feature  that allows them to be integrated.  MCM can determine study schedules that  maximize the durability of learning, and has implications for education  and training.  MCM can be cast either as a neural network with inputs that  fluctuate over time, or as a cascade of leaky integrators.  MCM is  intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, Shadmehr, 2007), yet MCM is better able to account for human  declarative memory.\",\n          \"Developing deep generative models has been an emerging field due to the ability to model and generate complex data for various purposes, such as image synthesis and molecular design. However, the advance of deep generative models is limited by the challenges to generate objects that possess multiple desired properties because: 1) the existence of complex correlation among real-world properties is common but hard to identify; 2) controlling individual property enforces an implicit partially control of its correlated properties, which is difficult to model; 3) controlling multiple properties under variour manners simultaneously is hard and underexplored. We address these challenges by proposing a novel deep generative framework that recovers semantics and correlation of properties through disentangled latent vectors. The correlation is handled via an explainable mask pooling layer, and properties are precisely retained by the generated objects via the mutual dependence between latent vectors and properties. Our generative model preserves properties of interest while handles correlation and conflicts of properties under a multi-objective optimization framework. The experiments demonstrate our model's superior performance in generating objects with desired properties.\",\n          \"We develop a hierarchical generative model to study cue combi(cid:173) nation.  The model maps a global shape parameter to local cue(cid:173) specific  parameters,  which in tum generate  an intensity image.  Inferring shape from images is achieved by inverting this model.  Inference produces a probability distribution at each level; using  distributions rather than a single value of underlying variables at  each stage preserves information about the validity  of each local  cue for the given image.  This allows the model, unlike standard  combination models, to adaptively weight each cue based on gen(cid:173) eral  cue  reliability  and  specific  image context.  We  describe  the  results  of a cue combination psychophysics experiment we con(cid:173) ducted that allows a direct comparison with the model. The model  provides a good fit to our data and a natural account for some in(cid:173) teresting aspects of cue combination. \\nUnderstanding cue  combination is  a  fundamental  step in developing  computa(cid:173) tional models of visual perception, because many aspects of perception naturally  involve multiple cues, such as binocular stereo, motion, texture, and shading. It is  often formulated as a problem of inferring or estimating some relevant parameter,  e.g., depth, shape, position, by combining estimates from individual cues.  An important finding  of psychophysical studies of cue combination is that cues  vary in the degree to which they are used in different visual environments. Weights  assigned to estimates derived from a particular cue seem to reflect  its estimated  reliability  in  the  current  scene  and  viewing  conditions.  For  example,  motion  and stereo are  weighted approximately equally at near distances, but motion is  weighted more at far distances,  presumably due to distance limits on binocular  disparity.3  Experiments have also found these weightings sensitive to image ma(cid:173) nipulations; if a cue is weakened, such as by adding noise, then the uncontami(cid:173) nated cue is utilized more in making depth judgments.9 A recent study2 has shown  that observers can adjust the weighting they assign to a cue based on its relative  utility for a particular task. From these and other experiments, we can identify two  types of information that determine relative cue weightings:  (1) cue reliability:  its  relative utility in the context of the task and general viewing conditions; and (2)  region informativeness:  cue information available locally in a given image.  A central question in computational models of cue combination then concerns how  these forms of uncertainty can be combined. We propose a hierarchical generative \\n870 \\nZ.  Yang and R.  S.  Zemel \\nmodel. Generative models have a rich history in cue combination, as thel underlie  models of Bayesian perception that have been developed in this area. lO ,  The nov(cid:173) elty in the generative model proposed here lies in its hierarchical nature and use  of distributions throughout, which allows for both context-dependent and image(cid:173) specific uncertainty to be combined in a principled manner.  Our aims in this paper are dual: to develop a combination model that incorporates  cue reliability and region informativeness (estimated across and within images),  and to use this model to account for data and provide predictions for psychophys(cid:173) ical experiments. Another motivation for the approach here stems from our recent  probabilistic framework,11  which posits that every step of processing entails  the  representation of an entire probability distribution, rather than just a single value  of the relevant underlying variable(s).  Here we use separate local probability dis(cid:173) tributions for each cue estimated directly from an image. Combination then entails  transforming representations and integrating distributions across both space and  cues, taking across- and within-image uncertainty into account. \\n1  IMAGE GENERATION \\nIn this paper we study the case of combining shading and texture. Standard shape(cid:173) from-shading models exclude texture, l, 8  while standard shape-from-texture mod(cid:173) els  exclude  shading.7  Experimental  results  and  computational arguments  have  supported a strong interaction between these cues}O but no model accounting for  this interaction has yet been worked out.  The shape used in our experiments is a simple surface: \\nZ = B(l - x2 ), Ixl  <= 1, Iyl  <= 1 \\n(1) \\nwhere Z is the height from the xy plane.  B is the only shape parameter.  Our image formation model is a hierarchical generative model (see Figure 1). The  top layer contains the global parameter B.  The second layer contains local shad(cid:173) ing and texture parameters S, T  = {Sj, 11}, where i  indexes image regions.  The  generation of local cues from a global parameter is intended to allow local uncer(cid:173) tainties to be introduced separately into the cues.  This models specific conditions  in realistic images, such as shading uncertainty due to shadows or specularities,  and texture uncertainty when prior assumptions such as isotropicity are violated.4  Here we introduce uncertainty by adding independent local noise to the underly(cid:173) ing shape parameter; this manipulation is less realistic but easier to control.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24779,\n        \"samples\": [\n          \"https://proceedings.neurips.cc/paper_files/paper/2022/file/c32319f4868da7613d78af9993100e42-Paper-Conference.pdf\",\n          \"https://proceedings.neurips.cc/paper_files/paper/2006/file/60792d855cd8a912a97711f91a1f155c-Paper.pdf\",\n          \"https://proceedings.neurips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24714,\n        \"samples\": [\n          \"COFIRANK\\nMaximum Margin Matrix Factorization for\\nCollaborative Ranking\\nMarkus Weimer\\u2217\\nAlexandros Karatzoglou\\u2020\\nQuoc Viet Le\\u2021\\nAlex Smola\\u00a7\\nAbstract\\nIn this paper, we consider collaborative \\ufb01ltering as a ranking problem. We present\\na method which uses Maximum Margin Matrix Factorization and optimizes rank-\\ning instead of rating. We employ structured output prediction to optimize directly\\nfor ranking scores. Experimental results show that our method gives very good\\nranking scores and scales well on collaborative \\ufb01ltering tasks.\\n1\\nIntroduction\\nCollaborative \\ufb01ltering has gained much attention in the machine learning community due to the\\nneed for it in webshops such as those of Amazon, Apple and Net\\ufb02ix. Webshops typically offer\\npersonalized recommendations to their customers. The quality of these suggestions is crucial to the\\noverall success of a webshop. However, suggesting the right items is a highly nontrivial task: (1)\\nThere are many items to choose from. (2) Customers only consider very few (typically in the order\\nof ten) recommendations. Collaborative \\ufb01ltering addresses this problem by learning the suggestion\\nfunction for a user from ratings provided by this and other users on items offered in the webshop.\\nThose ratings are typically collected on a \\ufb01ve star ordinal scale within the webshops.\\nLearning the suggestion function can be considered either a rating (classi\\ufb01cation) or a ranking prob-\\nlem. In the context of rating, one predicts the actual rating for an item that a customer has not rated\\nyet. On the other hand, for ranking, one predicts a preference ordering over the yet unrated items.\\nGiven the limited size of the suggestion shown to the customer, both (rating and ranking) are used\\nto compile a top-N list of recommendations. This list is the direct outcome of a ranking algorithm,\\nand can be computed from the results of a rating algorithm by sorting the items according to their\\npredicted rating. We argue that rating algorithms solve the wrong problem, and one that is actually\\nharder: The absolute value of the rating for an item is highly biased for different users, while the\\nranking is far less prone to this problem.\\nOne approach is to solve the rating problem using regression. For example for the Net\\ufb02ix prize\\nwhich uses root mean squared error as an evaluation criterion,1 the most straightforward approach\\nis to use regression. However, the same arguments discussed above apply to regression. Thus, we\\npresent an algorithm that solves the ranking problem directly, without \\ufb01rst computing the rating.\\nFor collaborative rating, Maximum Margin Matrix Factorization (MMMF) [11, 12, 10] has proven to\\nbe an effective means of estimating the rating function. MMMF takes advantage of the collaborative\\neffects: rating patterns from other users are used to estimate ratings for the current user. One key\\n\\u2217Telecooperation Group, TU Darmstadt, Germany, mweimer@tk.informatik.tu-darmstadt.de\\n\\u2020Department of Statistics, TU Wien, alexis@ci.tuwien.ac.at\\n\\u2021Computer Science Department, Stanford University, Stanford, CA 94305, quoc.le@stanford.edu\\n\\u00a7SML, NICTA, Northbourne Av. 218, Canberra 2601, ACT, Australia, alex.smola@nicta.com.au\\n1We conjecture that this is the case in order to keep the rules simple, since ranking scores are somewhat\\nnontrivial to de\\ufb01ne, and there are many different ways to evaluate a ranking, as we will see in the following.\\n1\\nadvantage of this approach is that it works without feature extraction. Feature extraction is domain\\nspeci\\ufb01c, e.g. the procedures developed for movies cannot be applied to books. Thus, it is hard\\nto come up with a consistent feature set in applications with many different types of items, as for\\nexample at Amazon. Our algorithm is based on this idea of MMMF, but optimizes ranking measures\\ninstead of rating measures.\\nGiven that only the top ranked items will actually be presented to the user, it is much more important\\nto rank the \\ufb01rst items right than the last ones. In other words, it is more important to predict what a\\nuser likes than what she dislikes. In more technical terms, the value of the error for estimation is not\\nuniform over the ratings. All of above reasonings lead to the following goals:\\n\\u2022 The algorithm needs to be able to optimize ranking scores directly.\\n\\u2022 The algorithm needs to be adaptable to different scores.\\n\\u2022 The algorithm should not require any features besides the actual ratings.\\n\\u2022 The algorithm needs to scale well and parallelize such as to deal with millions of ratings arising\\nfrom thousands of items and users with an acceptable memory footprint.\\nWe achieve these goals by combining (a) recent results in optimization, in particular the application\\nof bundle methods to convex optimization problems [14], (b) techniques for representing functions\\non matrices, in particular maximum margin matrix factorizations [10, 11, 12] and (c) the application\\nof structured estimation for ranking problems. We describe our algorithm COFIRANK in terms of\\noptimizing the ranking measure Normalized Discounted Cumulative Gain (NDCG).\\n2\\nProblem De\\ufb01nition\\nAssume that we have m items and u users. The ratings are stored in the sparse matrix Y where\\nYi,j \\u2208{0, . . . , r} is the rating of item j by user i and r is some maximal score. Yi,j is 0 if user\\ni did not rate item j. In rating, one estimates the missing values in Y directly while we treat this\\nas a ranking task. Additionally, in NDCG [16], the correct order of higher ranked items is more\\nimportant than that of lower ranked items:\\nDe\\ufb01nition 1 (NDCG) Denote by y \\u2208{1, . . . , r}n a vector of ratings and let \\u03c0 be a permutation\\nof that vector. \\u03c0i denotes the position of item i after the permutation. Moreover, let k \\u2208N be a\\ntruncation threshold and \\u03c0s sorts y in decreasing order. In this case the Discounted Cumulative\\nGains (DCG@k) score [5] and its normalized variant (NDCG@k) are given by\\nDCG@k(y, \\u03c0) =\\nk\\nX\\ni=1\\n2y\\u03c0i \\u22121\\nlog(i + 2)\\nand\\nNDCG@k(y, \\u03c0) = DCG@k(y, \\u03c0)\\nDCG@k(y, \\u03c0s)\\nDCG@k is maximized for \\u03c0 = \\u03c0s. The truncation threshold k re\\ufb02ects how many recommendations\\nusers are willing to consider. NDCG is a normalized version of DCG so that the score is bounded\\nby [0, 1].\\nUnlike classi\\ufb01cation and regression measures, DCG is de\\ufb01ned on permutations, not absolute val-\\nues of the ratings. Departing from traditional pairwise ranking measures [4], DCG is position-\\ndependent: Higher positions have more in\\ufb02uence on the score than lower positions. Optimizing\\nDCG has gained much interest in the machine learning and information retrieval (e.g. [2]) commu-\\nnities. However, we present the \\ufb01rst effort to optimize this measure for collaborative \\ufb01ltering.\\nTo perform estimation, we need a recipe for obtaining the permutations \\u03c0. Since we want our system\\nto be scalable, we need a method which scales not much worse than linearly in the number of the\\nitems to be ranked. The avenue we pursue is to estimate a matrix F \\u2208Rm\\u00d7u and to use the values\\nFij for the purpose of ranking the items j for user i. Given a matrix Y of known ratings we are now\\nable to de\\ufb01ne the performance of F:\\nR(F, Y ) :=\\nu\\nX\\ni=1\\nNDCG@k(\\u03a0i, Y i),\\n(1)\\n2\\nwhere \\u03a0i is argsort(\\u2212F i), it sorts F i in decreasing order.2 While we would like to maximize\\nR(F, Ytest) we only have access to R(F, Ytrain). Hence, we need to restrict the complexity of F to\\nensure good performance on the test set when maximizing the score on the training set.\\n3\\nStructured Estimation for Ranking\\nHowever, R(F, Y ) is non-convex. In fact, it is piecewise constant and therefore clearly not amenable\\nto any type of smooth optimization. To address this issue we take recourse to structured estimation\\n[13, 15]. Note that the scores decompose into a sum over individual users\\u2019 scores, hence we only\\nneed to show how minimizing \\u2212NDCG(\\u03c0, y) can be replaced by minimizing a convex upper bound\\non the latter. Summing over the users then provides us with a convex bound for all of the terms.3\\nOur conversion works in three steps:\\n1. Converting NDCG(\\u03c0, y) into a loss by computing the regret with respect to the optimal\\npermutation argsort(\\u2212y).\\n2. Denote by \\u03c0 a permutation (of the n items a user might want to see) and let f \\u2208Rn be a\\nestimated rating. We design a mapping \\u03c8(\\u03c0, f) \\u2192R which is linear in f in such a way\\nthat maximizing \\u03c8(\\u03c0, f) with respect to \\u03c0 yields argsort(f).\\n3. We use the convex upper-bounding technique described by [15] to combine regret and\\nlinear map into a convex upper bound which we can minimize ef\\ufb01ciently.\\nStep 1 (Regret Conversion)\\nInstead of maximizing NDCG(\\u03c0, y) we may also minimize\\n\\u2206(\\u03c0, y) := 1 \\u2212NDCG(\\u03c0, y).\\n(2)\\n\\u2206(\\u03c0, y) is nonnegative and vanishes for \\u03c0 = \\u03c0s.\\nStep 2 (Linear Mapping)\\nKey in our reasoning is the use of the Polya-Littlewood-Hardy inequal-\\nity: For any two vectors a, b \\u2208Rn their inner product is maximized by sorting a and b in the same\\norder, that is \\u27e8a, b\\u27e9\\u2264\\u27e8sort(a), sort(b)\\u27e9. This allows us to encode the permuation \\u03c0 = argsort(f)\\nin the following fashion: denote by c \\u2208Rn a decreasing nonnegative sequence, then the function\\n\\u03c8(\\u03c0, f) := \\u27e8c, f\\u03c0\\u27e9\\n(3)\\nis linear in f and maximized with respect to \\u03c0 for argsort(f). Since ci is decreasing by construction,\\nthe Polya-Littlewood-Hardy inequality applies. We found that choosing ci = (i + 1)\\u22120.25 produced\\ngood results in our experiments. However, we did not formally optimize this parameter.\\nStep 3 (Convex Upper Bound)\\nWe adapt a result of [15] which describes how to \\ufb01nd convex\\nupper bounds on nonconvex optimization problems.\\nLemma 2 Assume that \\u03c8 is de\\ufb01ned as in (3). Moreover let \\u03c0\\u2217:= argsort(\\u2212f) be the ranking\\ninduced by f. Then the following loss function l(f, y) is convex in f and it satis\\ufb01es l(f, y) \\u2265\\n\\u2206(y, \\u03c0\\u2217).\\nl(f, y) := max\\n\\u03c0\\nh\\n\\u2206(\\u03c0, y) + \\u27e8c, f\\u03c0 \\u2212f\\u27e9\\ni\\n(4)\\nProof We show convexity \\ufb01rst. The argument of the maximization over the permutations \\u03c0 is a\\nlinear and thus convex function in f. Taking the maximum over a set of convex functions is convex\\nitself, which proves the \\ufb01rst claim. To see that it is an upper bound, we use the fact that\\nl(f, y) \\u2265\\u2206(\\u03c0\\u2217, y) + \\u27e8c, f\\u03c0\\u2217\\u2212f\\u27e9\\u2265\\u2206(\\u03c0\\u2217, y).\\n(5)\\nThe second inequality follows from the fact that \\u03c0\\u2217maximizes \\u27e8c, f\\u03c0\\u2217\\u27e9.\\n2M i denotes row i of matrix M. Matrices are written in upper case, while vectors are written in lower case.\\n3This also opens the possibility for parallelization in the implementation of the algorithm.\\n3\\n4\\nMaximum Margin Matrix Factorization\\nLoss\\nThe reasoning in the previous section showed us how to replace the ranking score with a\\nconvex upper bound on a regret loss. This allows us to replace the problem of maximizing R(F, Y )\\nby that of minimizing a convex function in F, namely\\nL(F, Y ) :=\\nu\\nX\\ni=1\\nl(F i, Y i)\\n(6)\\nMatrix Regularization\\nHaving addressed the problem of non-convexity of the performance score\\nwe need to \\ufb01nd an ef\\ufb01cient way of performing capacity control of F, since we only have L(F, Ytrain)\\nat our disposition, whereas we would like to do well on L(F, Ytest). The idea to overcome this prob-\\nlem is by means of a regularizer on F, namely the one proposed for Maximum Margin Factorization\\nby Srebro and coworkers[10, 11, 12]. The key idea in their reasoning is to introduce a regularizer on\\nF via\\n\\u2126[F] := 1\\n2 min\\nM,U [tr MM \\u22a4+ tr UU \\u22a4] subject to UM = F.\\n(7)\\nMore speci\\ufb01cally, [12] show that the above is a proper norm on F. While we could use a semidef-\\ninite program as suggested in [11], the latter is intractable for anything but the smallest problems.4\\nInstead, we replace F by UM and solve the following problem:\\nminimize\\nM,U\\nL(UM, Ytrain) + \\u03bb\\n2\\n\\u0002\\ntr MM \\u22a4+ tr UU \\u22a4\\u0003\\n(8)\\nNote that the above matrix factorization approach effectively allows us to learn an item matrix M\\nand a user matrix U which will store the speci\\ufb01c properties of users and items respectively. This\\napproach learns the features of the items and the users. The dimension d of M \\u2208Rd\\u00d7m and\\nU \\u2208Rd\\u00d7u is chosen mainly based on computational concerns, since a full representation would\\nrequire d = min(m, u). On large problems the storage requirements for the user matrix can be\\nenormous and it is convenient to choose d = 10 or d = 100.\\nAlgorithm\\nWhile (8) may not be jointly convex in M and U any more, it still is convex in M and\\nU individually, whenever the other term is kept \\ufb01xed. We use this insight to perform alternating sub-\\nspace descent as proposed by [10]. Note that the algorithm does not guarantee global convergence,\\nwhich is a small price to pay for computational tractability.\\nrepeat\\nFor \\ufb01xed M minimize (8) with respect to U.\\nFor \\ufb01xed U minimize (8) with respect to M.\\nuntil No more progress is made or a maximum iteration count has been reached.\\nNote that on problems of the size of Net\\ufb02ix the matrix Y has 108 entries, which means that the\\nnumber of iterations is typically time limited. We now discuss a general optimization method for\\nsolving regularized convex optimization problems. For more details see [14].\\n5\\nOptimization\\nBundle Methods\\nWe discuss the optimization over the user matrix U \\ufb01rst, that is, consider the\\nproblem of minimizing\\nR(U) := L(UM, Ytrain) + \\u03bb\\n2 tr UU \\u22a4\\n(9)\\nThe regularizer tr UU \\u22a4is rather simple to compute and minimize. On the other hand, L is expensive\\nto compute, since it involves maximizing l for all users.\\nBundle methods, as proposed in [14] aim to overcome this problem by performing successive Taylor\\napproximations of L and by using them as lower bounds. In other words, they exploit the fact that\\nL(UM, Ytrain) \\u2265L(UM \\u2032, Ytrain) + tr(M \\u2212M \\u2032)\\u22a4\\u2202ML(UM \\u2032, Y )\\u2200M, M \\u2032.\\n4In this case we optimize over\\n\\u00bb A\\nF\\nF \\u22a4\\nB\\n\\u2013\\n\\u2ab00 where \\u2126[F] is replaced by 1\\n2[tr A + tr B].\\n4\\nAlgorithm 1 Bundle Method(\\u03f5)\\nInitialize t = 0, U0 = 0, b0 = 0 and H = \\u221e\\nrepeat\\nFind minimizer Ut and value L of the optimization problem\\nminimize\\nU\\nmax\\n0\\u2264j\\u2264t\\n\\u0002\\ntr U \\u22a4\\nj M + bj\\n\\u0003\\n+ \\u03bb\\n2 tr U \\u22a4U.\\nCompute Ut+1 = \\u2202UL(UtM, Ytrain)\\nCompute bt+1 = L(UtM, Ytrain) \\u2212tr Ut+1Mt\\nif H\\u2032 := tr U \\u22a4\\nt+1Mt + bt+1 + \\u03bb\\n2 tr UU \\u22a4\\u2264H then\\nUpdate H \\u2190H\\u2032\\nend if\\nuntil H \\u2212L \\u2264\\u03f5\\nSince this holds for arbitrary M \\u2032, we may pick a set of Mi and use the maximum over the Taylor\\napproximations at locations Mi to lower-bound L. Subsequently, we minimize this piecewise linear\\nlower bound in combination with \\u03bb\\n2 tr UU \\u22a4to obtain a new location where to compute our next\\nTaylor approximation and iterate until convergence is achieved. Algorithm 1 provides further details.\\nAs we proceed with the optimization, we obtain increasingly tight lower bounds on L(UM, Ytrain).\\nOne may show [14] that the algorithm converges to \\u03f5 precision with respect to the minimizer of\\nR(U) in O(1/\\u03f5) steps. Moreover, the initial distance from the optimal solution enters the bound\\nonly logarithmically.\\nAfter solving the optimization problem in U we switch to optimizing over the item matrix M. The\\nalgorithm is virtually identical to that in U, except that we now need to use the regularizer in M\\ninstead of that in U. We \\ufb01nd experimentally that a small number of iterations (less than 10) is more\\nthan suf\\ufb01cient for convergence.\\nComputing the Loss\\nSo far we simply used the loss l(f, y) of (4) to de\\ufb01ne a convex loss with-\\nout any concern to its computability. To implement Algorithm 1, however, we need to be able to\\nsolve the maximization of l with respect to the set of permutations \\u03c0 ef\\ufb01ciently. One may show\\nthat computing the \\u03c0 which maximizes l(f, y) is possible by solving the inear assignment problem\\nmin P\\ni\\nP\\nj Ci,jXi,j with the cost matrix:\\nCi,j = \\u03bai\\n2Y [j] \\u22121\\nDCG(Y, k, \\u03c0s)log(i + 1) \\u2212cifj with \\u03bai =\\n\\u001a1\\nif i < k,\\n0\\notherwise\\nEf\\ufb01cient algorithms [7] based on the Hungarian Marriage algorithm (also referred to as the Kuhn-\\nMunkres algorithm) exist for this problem [8]: it turns out that this integer programming problem\\ncan be solved by invoking a linear program. This in turn allows us to compute l(f, y) ef\\ufb01ciently.\\nComputing the Gradients\\nThe second ingredient needed for applying the bundle method is to\\ncompute the gradients of L(F, Y ) with respect to F, since this allows us to compute gradients with\\nrespect to M and U by applying the chain rule:\\n\\u2202ML(UM, Y ) = U \\u22a4\\u2202F L(X, F, Y ) and \\u2202UL(UM, Y ) = \\u2202F L(X, F, Y )\\u22a4M\\nL decomposes into losses on individual users as described in (6). For each user i only row i of F\\nmatters. It follows that \\u2202F L(F, Y ) is composed of the gradients of l(F i, Y i). Note that for l de\\ufb01ned\\nas in (4) we know that\\n\\u2202F il(F i, Y i) = [c \\u2212c\\u00af\\u03c0\\u22121].\\nHere we denote by \\u00af\\u03c0 the maximizer of of the loss and c\\u00af\\u03c0\\u22121 denotes the application of the inverse\\npermutation \\u00af\\u03c0\\u22121 to the vector c.\\n5\\n6\\nExperiments\\nWe evaluated COFIRANK with the NDCG loss just de\\ufb01ned (denoted by COFIRANK-NDCG) as\\nwell as with loss functions which optimize ordinal regression (COFIRANK-Ordinal) and regression\\n(COFIRANK-Regression). COFIRANK-Ordinal applies the algorithm described above to preference\\nranking by optimizing the preference ranking loss. Similarly, COFIRANK-Regression optimizes for\\nregression using the root mean squared loss. We looked at two real world evaluation settings: \\u201cweak\\u201d\\nand \\u201cstrong\\u201d [9] generalization on three publicly available data sets: EachMovie, MovieLens and\\nNet\\ufb02ix. Statistics for those can be found in table 1.\\nDataset\\nUsers\\nMovies\\nRatings\\nEachMovie\\n61265\\n1623\\n2811717\\nMovieLens\\n983\\n1682\\n100000\\nNet\\ufb02ix\\n480189\\n17770\\n100480507\\nTable 1: Data set statistics\\nWeak generalization\\nis evaluated by predicting the rank of unrated items for users known at\\ntraining time. To do so, we randomly select N = 10, 20, 50 ratings for each user for training and\\nand evaluate on the remaining ratings. Users with less then 20, 30, 60 rated movies where removed\\nto ensure that the we could evaluate on at least 10 movies per user We compare COFIRANK-NDCG,\\nCOFIRANK-Ordinal, COFIRANK-Regression and MMMF [10]. Experimental results are shown in\\ntable 2.\\nFor all COFIRANK experiments, we choose \\u03bb = 10. We did not optimize for this parameter. The\\nresults for MMMF were obtained using MATLAB code available from the homepage of the authors\\nof [10]. For those, we used \\u03bb =\\n1\\n1.9 for EachMovie, and \\u03bb =\\n1\\n1.6 for MovieLens as it is reported\\nto yield the best results for MMMF. In all experiments, we choose the dimensionality of U and M\\nto be 100. All COFIRANK experiments and those of MMMF on MovieLens were repeated ten times.\\nUnfortunately, we underestimated the runtime and memory requirements of MMMF on EachMovie.\\nThus, we cannot report results on this data set using MMMF.\\nAdditionally, we performed some experiments on the Net\\ufb02ix data set. However, we cannot compare\\nto any of the other methods on that data set as to the best of our knowledge, COFIRANK is the \\ufb01rst\\ncollaborative ranking algorithm to be applied to this data set, supposedly because of its large size.\\nStrong generalization\\nis evaluated on users that were not present at training time. We follow the\\nprocedure described in [17]: Movies with less than 50 ratings are discarded. The 100 users with the\\nmost rated movies are selected as the test set and the methods are trained on the remaining users.\\nIn evaluation, 10, 20 or 50 ratings from those of the 100 test users are selected. For those ratings,\\nthe user training procedure is applied to optimize U. M is kept \\ufb01xed in this process to the values\\nobtained during training. The remaining ratings are tested using the same procedure as for the weak\\nMethod\\nN=10\\nN=20\\nN=50\\nEachMovie\\nCOFIRANK-NDCG\\n0.6562 \\u00b1 0.0012\\n0.6644 \\u00b1 0.0024\\n0.6406 \\u00b1 0.0040\\nCOFIRANK-Ordinal\\n0.6727 \\u00b1 0.0309\\n0.7240 \\u00b1 0.0018\\n0.7214 \\u00b1 0.0076\\nCOFIRANK-Regression\\n0.6114 \\u00b1 0.0217\\n0.6400 \\u00b1 0.0354\\n0.5693 \\u00b1 0.0428\\nMovieLens\\nCOFIRANK-NDCG\\n0.6400 \\u00b1 0.0061\\n0.6307 \\u00b1 0.0062\\n0.6076 \\u00b1 0.0077\\nCOFIRANK-Ordinal\\n0.6233 \\u00b1 0.0039\\n0.6686 \\u00b1 0.0058\\n0.7169 \\u00b1 0.0059\\nCOFIRANK-Regression\\n0.6420 \\u00b1 0.0252\\n0.6509 \\u00b1 0.0190\\n0.6584 \\u00b1 0.0187\\nMMMF\\n0.6061 \\u00b1 0.0037\\n0.6937 \\u00b1 0.0039\\n0.6989 \\u00b1 0.0051\\nNet\\ufb02ix\\nCOFIRANK-NDCG\\n0.6081\\n0.6204\\nCOFIRANK-Regression\\n0.6082\\n0.6287\\nTable 2: Results for the weak generalization setting experiments. We report the NDCG@10 accuracy for\\nvarious numbers of training ratings used per user. For most results we report the mean over ten runs and the\\nstandard deviation. We also report the p-values for the best vs. second best score.\\n6\\nMethod\\nN=10\\nN=20\\nN=50\\nEachMovie\\nCOFIRANK-NDCG\\n0.6367 \\u00b1 0.001\\n0.6619 \\u00b1 0.0022\\n0.6771 \\u00b1 0.0019\\nGPR\\n0.4558 \\u00b1 0.015\\n0.4849 \\u00b1 0.0066\\n0.5375 \\u00b1 0.0089\\nCGPR\\n0.5734 \\u00b1 0.014\\n0.5989 \\u00b1 0.0118\\n0.6341 \\u00b1 0.0114\\nGPOR\\n0.3692 \\u00b1 0.002\\n0.3678 \\u00b1 0.0030\\n0.3663 \\u00b1 0.0024\\nCGPOR\\n0.3789 \\u00b1 0.011\\n0.3781 \\u00b1 0.0056\\n0.3774 \\u00b1 0.0041\\nMMMF\\n0.4746 \\u00b1 0.034\\n0.4786 \\u00b1 0.0139\\n0.5478 \\u00b1 0.0211\\nMovieLens\\nCOFIRANK-NDCG\\n0.6237 \\u00b1 0.0241\\n0.6711 \\u00b1 0.0065\\n0.6455 \\u00b1 0.0103\\nGPR\\n0.4937 \\u00b1 0.0108\\n0.5020 \\u00b1 0.0089\\n0.5088 \\u00b1 0.0141\\nCGPR\\n0.5101 \\u00b1 0.0081\\n0.5249 \\u00b1 0.0073\\n0.5438 \\u00b1 0.0063\\nGPOR\\n0.4988 \\u00b1 0.0035\\n0.5004 \\u00b1 0.0046\\n0.5011 \\u00b1 0.0051\\nCGPOR\\n0.5053 \\u00b1 0.0047\\n0.5089 \\u00b1 0.0044\\n0.5049 \\u00b1 0.0035\\nMMMF\\n0.5521 \\u00b1 0.0183\\n0.6133 \\u00b1 0.0180\\n0.6651 \\u00b1 0.0190\\nTable 3: The NGDC@10 accuracy over ten runs and the standard deviation for the strong generalization eval-\\nuation.\\ngeneralization. We repeat the whole process 10 times and again use \\u03bb = 10 and a dimensionality of\\n100. We compare COFIRANK-NDCG to Gaussian Process Ordinal Regression (GPOR) [3] Gaussian\\nProcess Regression (GPR) and the collaborative extensions (CPR, CGPOR) [17]. Table 3 shows our\\nresults compared to the ones from [17].\\nCOFIRANK performs strongly compared to most of the other tested methods. Particularly in the strong\\ngeneralization setting COFIRANK outperforms the existing methods in almost all the settings. Note\\nthat all methods except COFIRANK and MMMF use additional extracted features which are either\\nprovided with the dataset or extracted from the IMDB. MMMF and COFIRANK only rely on the\\nrating matrix. In the weak generalization experiments on the MovieLens data, COFIRANK performs\\nbetter for N = 20 but is marginally outperformed by MMMF for the N = 10 and N = 50 cases.\\nWe believe that with proper parameter tuning, COFIRANK will perform better in these cases.\\n7\\nDiscussion and Summary\\nCOFIRANK is a novel approach to collaborative \\ufb01ltering which solves the ranking problem faced\\nby webshops directly. It can do so faster and at a higher accuracy than approaches which learn\\na rating to produce a ranking. COFIRANK is adaptable to different loss functions such as NDCG,\\nRegression and Ordinal Regression in a plug-and-play manner. Additionally, COFIRANK is well\\nsuited for privacy concerned applications, as the optimization itself does not need ratings from the\\nusers, but only gradients.\\nOur results, which we obtained without parameters tuning, are on par or outperform several of the\\nmost successful approaches to collaborative \\ufb01ltering like MMMF, even when they are used with\\ntuned parameters. COFIRANK performs best on data sets of realistic sizes such as EachMovie and\\nsigni\\ufb01cantly outperforms other approaches in the strong generalization setting.\\nIn our experiments, COFIRANKshows to be very fast. For example, training on EachMovie with\\nN = 10 can be done in less than ten minutes and uses less than 80MB of memory on a laptop. For\\nN = 20, COFIRANK obtained a NDCG@10 of 0.72 after the \\ufb01rst iteration, which also took less than\\nten minutes. This is the highest NDCG@10 score on that data set we are aware of (apart from the\\nresult of COFIRANK after convergence). A comparison to MMMF in that regard is dif\\ufb01cult, as it is\\nimplemented in MATLAB and COFIRANK in C++. However, COFIRANK is more than ten times faster\\nthan MMMF while using far less memory. In the future, we will exploit the fact that the algorithm\\nis easily parallelizable to obtain even better performance on current multi-core hardware as well as\\ncomputer clusters. Even the current implementation allows us to report the \\ufb01rst results on the Net\\ufb02ix\\ndata set for direct ranking optimization.\\nAcknowledgments: Markus Weimer is funded by the German Research Foundation as part of the Research\\nTraining Group 1223: \\u201cFeedback-Based Quality Management in eLearning\\u201d.\\nSoftware: COFIRANK is available from http://www.cofirank.org\\n7\\nReferences\\n[2] C. J. Burges, Q. V. Le, and R. Ragno. Learning to rank with nonsmooth cost functions. In\\nB. Sch\\u00a8olkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing\\nSystems 19, 2007.\\n[3] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Mach. Learn. Res.,\\n6:1019\\u20131041, 2005.\\n[4] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regres-\\nsion. In A. J. Smola, P. L. Bartlett, B. Sch\\u00a8olkopf, and D. Schuurmans, editors, Advances in\\nLarge Margin Classi\\ufb01ers, pages 115\\u2013132, Cambridge, MA, 2000. MIT Press.\\n[5] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents.\\nIn ACM Special Interest Group in Information Retrieval (SIGIR), pages 41\\u201348. New York:\\nACM, 2002.\\n[7] R. Jonker and A. Volgenant. A shortest augmenting path algorithm for dense and sparse linear\\nassignment problems. Computing, 38:325\\u2013340, 1987.\\n[8] H.W. Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics\\nQuarterly, 2:83\\u201397, 1955.\\n[9] B. Marlin. Collaborative \\ufb01ltering: A machine learning perspective. Masters thesis, University\\nof Toronto, 2004.\\n[10] J. Rennie and N. Srebro. Fast maximum margin matrix factoriazation for collaborative predic-\\ntion. In Proc. Intl. Conf. Machine Learning, 2005.\\n[11] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In L. K. Saul,\\nY. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17,\\nCambridge, MA, 2005. MIT Press.\\n[12] N. Srebro and A. Shraibman.\\nRank, trace-norm and max-norm.\\nIn P. Auer and R. Meir,\\neditors, Proc. Annual Conf. Computational Learning Theory, number 3559 in Lecture Notes\\nin Arti\\ufb01cial Intelligence, pages 545\\u2013560. Springer-Verlag, June 2005.\\n[13] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In S. Thrun, L. Saul, and\\nB. Sch\\u00a8olkopf, editors, Advances in Neural Information Processing Systems 16, pages 25\\u201332,\\nCambridge, MA, 2004. MIT Press.\\n[14] C.H. Teo, Q. Le, A.J. Smola, and S.V.N. Vishwanathan. A scalable modular convex solver\\nfor regularized risk minimization. In Conference on Knowledge Discovery and Data Mining,\\n2007.\\n[15] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured\\nand interdependent output variables. J. Mach. Learn. Res., 6:1453\\u20131484, 2005.\\n[16] E. Voorhees. Overview of the TREC 2001 question answering track. In Text REtrieval Con-\\nference (TREC) Proceedings. Department of Commerce, National Institute of Standards and\\nTechnology, 2001. NIST Special Publication 500-250: The Tenth Text REtrieval Conference\\n(TREC 2001).\\n[17] S. Yu, K. Yu, V. Tresp, and H. P. Kriegel. Collaborative ordinal regression. In W.W. Cohen\\nand A. Moore, editors, Proc. Intl. Conf. Machine Learning, pages 1089\\u20131096. ACM, 2006.\\n8\\n\",\n          \"NanoFlow: Scalable Normalizing Flows with\\nSublinear Parameter Complexity\\nSang-gil Lee\\nSungwon Kim\\nSungroh Yoon\\u2217\\nData Science & AI Lab.\\nSeoul National University\\n{tkdrlf9202, ksw0306, sryoon}@snu.ac.kr\\nAbstract\\nNormalizing \\ufb02ows (NFs) have become a prominent method for deep generative\\nmodels that allow for an analytic probability density estimation and ef\\ufb01cient syn-\\nthesis. However, a \\ufb02ow-based network is considered to be inef\\ufb01cient in parameter\\ncomplexity because of reduced expressiveness of bijective mapping, which renders\\nthe models unfeasibly expensive in terms of parameters. We present an alternative\\nparameterization scheme called NanoFlow, which uses a single neural density\\nestimator to model multiple transformation stages. Hence, we propose an ef\\ufb01cient\\nparameter decomposition method and the concept of \\ufb02ow indication embedding,\\nwhich are key missing components that enable density estimation from a single\\nneural network. Experiments performed on audio and image models con\\ufb01rm that\\nour method provides a new parameter-ef\\ufb01cient solution for scalable NFs with\\nsigni\\ufb01cant sublinear parameter complexity.\\n1\\nIntroduction\\nFlow-based models have become a prominent approach for density estimation and generative models\\nin recent times. These models are based on normalizing \\ufb02ows (NFs) [27], wherein a deep invertible\\nmodel is trained with an analytically estimated likelihood of data. The model learns a bijective\\nmapping between the data and a known prior (typically isotropic Gaussian), and its reverse operation\\nsynthesizes realistic samples from the prior. Compared with the variational autoencoder [18] and\\ngenerative adversarial network [10], NFs exhibit the distinct characteristic of an exact probability\\ndensity estimation from a principled maximum likelihood training. When combined with non-\\nautoregressive coupling methods [6, 19], NFs become a powerful generative model that offers a\\nsigni\\ufb01cantly simpli\\ufb01ed training and ef\\ufb01cient inference.\\nSince the introduction of the framework into neural networks, the existing studies on \\ufb02ow-based\\nmodels have focused on improving the expressiveness of the bijective operation [2,7,12,19]. However,\\nparameter complexity and memory ef\\ufb01ciency are less emphasized by the research community.\\nThe small efforts to maximize the expressiveness under a speci\\ufb01ed amount of capacity of the\\nneural network has recently become problematic when expanding a \\ufb02ow-based model for real-\\nworld applications. A notable example is the waveform synthesis model [15, 26]. Although the\\naforementioned studies have achieved audio generation faster than real-time (thereby removing the\\nslow inference bottleneck of WaveNet [29]), they resulted in an increase in the number of parameters\\nby an order of magnitude, which is unfeasibly expensive in terms of memory. Hence, building a\\ncomplex, scalable, and memory-ef\\ufb01cient \\ufb02ow-based model remains challenging.\\nThis scenario raised a question: Is it true that NFs require a signi\\ufb01cantly larger network capacity to\\nperform expressive bijections, or is the representational power of deep neural networks inef\\ufb01ciently\\n\\u2217Corresponding author\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\\nutilized? We argue that studies regarding NFs should consider the parameter complexity, where the\\nexpressiveness of multiple \\ufb02ows is not necessarily accompanied by a linearly growing number of\\nparameters.\\nIn this study, we challenge the typical assumption in building \\ufb02ow-based models and aim to decouple\\nthe required number of parameters and the expressiveness of multiple bijective operations for \\ufb02ow-\\nbased models. We present NanoFlow, an alternative parameterization scheme for NFs that operates\\non a single neural density estimator. Because the shared density estimator is applied to multiple\\nstacks of \\ufb02ows, the parameter requirement is no longer proportional to the number of \\ufb02ows, and the\\nmemory footprint is signi\\ufb01cantly reduced. Consequently, NanoFlow can consistently improve its\\nexpressiveness by stacking \\ufb02ows without sacri\\ufb01cing parameter ef\\ufb01ciency.\\nOur results indicated that using a conventional notion of weight sharing did not yield a good\\nperformance on \\ufb02ow-based models, which nulli\\ufb01es the potential bene\\ufb01ts. To achieve the concept of a\\nshared neural density estimator, we demonstrate several parameter-ef\\ufb01cient solutions for increasing\\nthe \\ufb02exibility of NanoFlow. We show that decomposing a deep hidden representation estimated\\nby the shared neural network and the projected densities from the representation can signi\\ufb01cantly\\nenhance the expressiveness of NanoFlow with the addition of a few parameters. Furthermore, we\\nalso demonstrate that conditioning the shared estimator with our \\ufb02ow indication embedding can\\nremedy the modeling dif\\ufb01culties of multiple densities from a single estimator without dissatisfying\\nany invertibility constraints.\\nAdditionally, we provide a deeper analysis of the condition under which our method yields the a\\nhigher number of bene\\ufb01ts. Speci\\ufb01cally, we assess the effectiveness of the single density estimator\\nby varying the amount of autoregressive structural bias into the model. Our results demonstrate that\\nour method performs best on bipartite \\ufb02ows, which provides an expanded narrative on our belief\\nregarding the performance gap between non-autoregressive and autoregressive models. In summary,\\nour study is the \\ufb01rst to focus on a systematic assessment for enabling scalable NFs with an almost\\nconstant parameter complexity.\\n2\\nBackground\\nNFs learn the bijective mapping between data and a known prior. The prior is typically constructed\\nas an isotropic Gaussian, and the reverse of the bijective mapping can synthesize the data from the\\nnoise sampled from the prior. Formally, NFs learn the bijective function f(x) = z, which transforms\\na complex data probability distribution PX into a simple known prior PZ with the same dimension.\\nWe can analytically compute the probability density of real data x using the change of variables\\nformula:\\nlog PX(x) = log PZ(z) + log | det(\\u2202f(x)\\n\\u2202x\\n)|,\\n(1)\\nwhere det( \\u2202f(x)\\n\\u2202x ) is a Jacobian determinant of the function f(x) = z. To enhance the expressiveness\\nof f, NF models decompose the function into multiple \\ufb02ows as follows:\\nf = f K \\u25e6f K\\u22121 \\u25e6... \\u25e6f 1(x),\\n(2)\\nwhere K is the number of \\ufb02ows de\\ufb01ned by the model. Using the notations x = z0 and z = zK, each\\nf k(zk\\u22121) = zk learns the intermediate densities between x and z, and log PX can be re-expressed\\nas follows:\\nlog PX(x) = log PZ(z) +\\nK\\nX\\nk=1\\nlog | det(\\u2202f k(zk\\u22121)\\n\\u2202zk\\u22121\\n)|.\\n(3)\\nBecause the determinant typically requires O(n3) computing time (where n is the dimension of the\\ndata), NF models are designed to maintain a triangular Jacobian [5,17,23]. By maintaining a triangular\\nJacobian, the determinant becomes easy to compute, and the model becomes computationally tractable\\nfor both forward and inverse functions.\\nOur mathematical notation for the coupling transformation follows that of the WaveFlow [25].\\nAlthough the study focused on waveform synthesis, it provides a uni\\ufb01ed view from bipartite to\\nautoregressive \\ufb02ows, which subsumes a wide range of \\ufb02ow-based models. We note that [23] also\\nprovides a relevant analysis regarding the relationship between autoregressive and bipartite \\ufb02ows.\\n2\\nDensity Es\\u019fmator\\nA\\ufb03ne \\nTransforma\\u019fon\\n,\\n,\\n(a) Baseline\\nShared\\nDensity Es\\u019fmator\\nA\\ufb03ne \\nTransforma\\u019fon\\n,\\n,\\n(b) NanoFlow-naive\\nFlow Indication\\nEmbedding\\n\\u2026\\n\\u2026\\nShared\\nDensity Es\\u019fmator\\nA\\ufb03ne \\nTransforma\\u019fon\\n,\\nProjec\\u019fon \\n,\\n(c) NanoFlow\\nFigure 1: High-level overview of NanoFlow. (a) Conventional NFs employ separate neural networks\\nas a density estimator for each \\ufb02ow. (b) NanoFlow-naive shares an entire part of the neural network\\nfor density estimation with multiple \\ufb02ow steps. (c) NanoFlow decomposes the estimator into two\\nparts\\u2014one for the deep shared latent space representation augmented by \\ufb02ow indication embedding,\\nand another for separate projection layers employed to each \\ufb02ow.\\nFormally, for training data x, assume that we split x into G groups as {X1, ..., XG}. The model is\\ntrained to learn the bijective mapping between X and a prior Z with the same dimension. This is\\nachieved by applying an af\\ufb01ne transformation f : X \\u2192Z which models a sequential dependency\\nbetween the grouped data as follows:\\nZi = \\u03c3i(X<i; \\u03b8) \\u00b7 Xi + \\u00b5i(X<i; \\u03b8),\\ni = 1, ..., G,\\n(4)\\nwhere X<i refers to all the partitions of the data before the i-th group, Xi; \\u03c3 and \\u00b5 are the scale and\\nshift variables estimated by the neural networks, respectively. From the sampled noise Z, the inverse\\nof the af\\ufb01ne transformation f \\u22121 : Z \\u2192X sequentially generates X as follows:\\nXi = Zi \\u2212\\u00b5i(X<i; \\u03b8)\\n\\u03c3i(X<i; \\u03b8)\\n,\\ni = 1, 2, ..., G.\\n(5)\\nThe model becomes a purely autoregressive \\ufb02ow when G = dim(x). Conversely, the equations\\ntheoretically represent bipartite \\ufb02ows when G = 2. Increasing the number of groups introduces a\\nhigher amount of autoregressive structural bias into the model, at a cost of O(G) inference latency.\\nAs previously mentioned, the entire bijective function f : X \\u2192Z is decomposed into K \\ufb02ows\\nas f = f K \\u25e6f K\\u22121 \\u25e6... \\u25e6f 1(X), where we use the notation X = Z0 and Z = ZK. Each\\nf k : Zk\\u22121 \\u2192Zk is parameterized by separate neural networks \\u03b8k, whereas each \\u03b8k estimates the\\nintermediate density of Zk by computing \\u03c3k and \\u00b5k for the \\ufb02ow operation. For clarity, we consider\\nthe notation of X as the input and Z as the output for f k. We re-write f k for completeness as\\nfollows:\\nZi = \\u03c3i(X<i; \\u03b8k) \\u00b7 Xi + \\u00b5i(X<i; \\u03b8k).\\n(6)\\nThe above formula describes the af\\ufb01ne transformation as a bijective coupling operation. Various\\nother classes of coupling exist in the literature [7,12].\\n3\\nNanoFlow\\nIn this section, we present NanoFlow, a new alternative parameterization scheme for a \\ufb02ow-based\\nmodel (Figure 1). The main goal of NanoFlow is to decouple the expressiveness of the bijections\\nand the parameter ef\\ufb01ciency of density estimation from neural networks. We initially describe a core\\nchange in the design of the neural architecture by decomposing the parameters for neural density\\nestimation and sharing parameters across \\ufb02ows.\\n3.1\\nParameter sharing and decomposition\\nWe reformulated f k\\n\\u00b5,\\u03c3 using a single shared neural network f\\u00b5,\\u03c3, parameterized by \\u03b8. Based on\\nthis framework, all \\u03c3k and \\u00b5k for each \\ufb02ow were estimated by the shared f\\u00b5,\\u03c3. This formulation\\n3\\nTable 1: Comparison of parameterization scheme of f k between methods for bijection. K is the total\\nnumber of \\ufb02ows de\\ufb01ned by the model, and | \\u2022 | is the number of parameters of the neural network\\nwith the designated letters.\\nMethod\\nf k : Xi \\u2192Zi = \\u03c3k\\ni \\u00b7 Xi + \\u00b5k\\ni , i = 1, ..., G\\nParameters\\nWaveFlow (baseline)\\n\\u00b5k\\ni , \\u03c3k\\ni = f k\\n\\u00b5,\\u03c3(X<i; \\u03b8k)\\nPK\\nk=1 |\\u03b8k|\\nNanoFlow-naive\\n\\u00b5k\\ni , \\u03c3k\\ni = f\\u00b5,\\u03c3(X<i; \\u03b8)\\n|\\u03b8|\\nNanoFlow-decomp\\n\\u00b5k\\ni , \\u03c3k\\ni = f k\\n\\u00b5,\\u03c3(g(X<i; \\u02c6\\u03b8); \\u03f5k)\\n|\\u02c6\\u03b8| + PK\\nk=1 |\\u03f5k|\\nNanoFlow (proposed)\\n\\u00b5k\\ni , \\u03c3k\\ni = f k\\n\\u00b5,\\u03c3(gk(X<i; \\u02c6\\u03b8, ek); \\u03f5k)\\n|\\u02c6\\u03b8| + PK\\nk=1(|\\u03f5k| + |ek|)\\ncan reduce the number of parameters by a fraction of the number of \\ufb02ows by 1\\nK , and we call this\\nvariant, the NanoFlow-naive. However, as our experimental results suggest, this aggressive re-use\\nof parameters is unsuitable for modeling multiple densities that suffer from severe degradation in\\nperformance, as it completely nulli\\ufb01es the potential bene\\ufb01t of the O(1) memory footprint.\\nBased on these observations, we propose to relax the constraint of the shared estimator by decompos-\\ning the shared model into a network that computes a hidden representation and a projection layer that\\nestimates the densities. The function is decomposed into f k\\n\\u00b5,\\u03c3 \\u25e6g, where g(\\u00b7; \\u02c6\\u03b8) is the shared estimator\\nparameterized by \\u02c6\\u03b8, excluding the projection layer, that is, each f k\\n\\u00b5,\\u03c3 has separate parameters for the\\nprojected intermediate density by computing \\u03c3k and \\u00b5k.\\nAssuming that g has suf\\ufb01cient capacity for density estimation, the projection layer can be as shallow\\nas a 1 \\u00d7 1 convolution; hence, the number of parameters is negligible in comparison with \\u02c6\\u03b8. Using\\nthis decomposition, we can construct NanoFlow with an arbitrary number of \\ufb02ows. Interestingly,\\nthis alternative scheme was pivotal for achieving the competitive performance of NanoFlow. We\\nobserved that the likelihood of the data continuously increased as we stacked additional \\ufb02ows without\\nsacri\\ufb01cing the ef\\ufb01ciency of weight sharing. Hence, we can re-write f k as follows:\\nZi = \\u03c3i(X<i; \\u02c6\\u03b8, \\u03f5k) \\u00b7 Xi + \\u00b5i(X<i; \\u02c6\\u03b8, \\u03f5k),\\n(7)\\nwhere \\u03f5k is the parameter of the separate projection layer assigned to each \\ufb02ow.\\n3.2\\nFlow indication embedding\\nEven when the parameter decomposition described above is used, the shared estimator g must learn\\nmultiple intermediate densities of bijective transformations without context. Hence, we introduce\\na key missing module, which we name \\ufb02ow indication embedding, to enable the shared model to\\nsimultaneously learn multiple bijective transformations. Because the \\ufb02ow-based model is based\\non the bijective function, the embedding must be available a priori for application to the reverse\\noperation.\\nFor each f k, we de\\ufb01ne an embedding vector ek \\u2208RD, where D is the dimension of the embedding.\\nSubsequently, we feed the embedding to the shared model g(\\u00b7; \\u02c6\\u03b8) as an additional context. From the\\nembedding ek, we can further guide g(\\u00b7; \\u02c6\\u03b8) to learn multiple intermediate densities with minimal\\naddition of parameters, transforming it into gk(\\u00b7; \\u02c6\\u03b8, ek). Because the order of \\ufb02ow operations is\\npre-de\\ufb01ned, we can use ek by feeding it to the shared estimator in the reverse order during inference,\\nthat is, our embedding does not dissatisfy any invertibility constraints.\\nThe optimal injection of the embedding into gk may depend on the neural architecture. We investigated\\nthree candidates: 1. concatenative embedding, in which we augment the input with the embedding\\nvector at the start of each \\ufb02ow; 2. additive bias, in which for each layer inside gk, ek provides a\\nchannel-wise bias projected from additionally de\\ufb01ned 1 \\u00d7 1 convolutional layers; 3. multiplicative\\ngating, in which we employ independent per-channel scalars inside gk for each \\ufb02ow that controls\\nthe propagation of the convolutional feature map according to the speci\\ufb01ed \\ufb02ow steps. Note that\\nthe aforementioned methods involve a negligible number of additional parameters, do not dissatisfy\\nthe invertibility, and impose a minimal effect on the inference latency. See Appendix A for a more\\ndetailed description.\\n4\\nTable 2: Model comparison. We report the number of model parameters in millions (M), a log-\\nlikelihood (LL) on the test set, a subjective \\ufb01ve-scale mean opinion score (MOS) on naturalness with\\n95 % con\\ufb01dence interval, and a synthesis speed using a single Nvidia V100 GPU with half-precision\\narithmetic. MOS on ground-truth audio is 4.58 \\u00b1 0.06.\\nMethod\\nChannels\\nParameters (M)\\nLL\\nMOS\\nkHz\\nWaveFlow (Re-impl)\\n128\\n22.336\\n5.2059\\n4.11 \\u00b1 0.08\\n347\\nWaveFlow (Re-impl)\\n64\\n5.925\\n5.1357\\n3.52 \\u00b1 0.09\\n828\\nNanoFlow-naive\\n128\\n2.792\\n5.1247\\n3.23 \\u00b1 0.09\\n376\\nNanoFlow\\n128\\n2.819\\n5.1586\\n3.63 \\u00b1 0.09\\n362\\nNanoFlow (K=16)\\n128\\n2.845\\n5.1873\\n3.82 \\u00b1 0.08\\n186\\nTable 1 summarizes the parameterization scheme of f k and its complexity. The parameter ef\\ufb01ciency\\nof NanoFlow is due to employing a single neural density estimator, g, for multiple \\ufb02ow operations.\\nNanoFlow-naive incorporates a conventional weight sharing and NanoFlow-decomp relaxes the\\nconstraint of intermediate density estimations by employing separate \\u03f5k for each \\ufb02ow. The \\ufb01nal\\nproposed model, NanoFlow, further increases the parameter ef\\ufb01ciency of gk by incorporating the\\n\\ufb02ow indication embedding ek. We emphasize that \\u02c6\\u03b8 embodies the majority of the parameters from\\nthe model.\\n4\\nExperiments\\nIn this section, we present a systematic assessment of the effectiveness of NanoFlow. We initially\\npresent the experimental results from an audio generative model with WaveFlow [25] as the baseline\\narchitecture, combined with an extensive ablation study. Next, we provide a likelihood ratio analysis\\nof NanoFlow by varying the amount of autoregressive structural bias into both models, which\\nevaluates the conditions under which NanoFlow yields more bene\\ufb01ts. Finally, we investigate the\\ngeneralizability of our methods by performing density modeling on the image domain, with Glow [19]\\nas the reference model.\\n4.1\\nAudio generation results\\nFor the performance evaluation of waveform generation, we used the LJ speech dataset [14], which\\nis a 24-h single-speaker speech dataset containing 13,100 audio clips. We used the \\ufb01rst 10% of the\\naudio clips as the test set and the remaining 90% as the training set. We used the audio preprocessing\\nand mel-spectrogram construction pipeline provided by the of\\ufb01cial WaveGlow implementation [26].\\nSpeci\\ufb01cally, we used an 80-band log-scale mel-spectrogram condition with an FFT size of 1,024,\\na hop size of 256, and a window size of 1,024. We used a maximum frequency of 8,000Hz for the\\nSTFT without audio volume normalization, and we set the noise distribution to Zi \\u223cN(0, 1), which\\nis the default setting from the open-source WaveGlow.\\nWe used the default architecture described in [25] with G = 16 for WaveFlow and NanoFlow.\\nWe constructed the models with eight \\ufb02ows unless otherwise speci\\ufb01ed and used the permutation\\nstrategy of reversing the order of the group dimensions per \\ufb02ow for both models. Our selection of\\n\\ufb02ow indication embedding is a combination of additive bias and multiplicative gating, as WaveNet-\\nbased [29] architecture features a natural method of utilizing additive bias as global conditioning\\naugmented by a gated residual path [30]. We used D = 512 for ek \\u2208RD in the default eight-\\ufb02ows\\nmodel and D = 1024 in the 16-\\ufb02ows variant.\\nWe trained all models for 1.2 M iterations with a batch size of eight and an audio clip size of 16,000,\\nusing an Nvidia V100 GPU. We used the Adam optimizer [16] with an initial learning rate of 10\\u22123,\\nand we annealed the learning rate by half for every 200 K iterations. For the evaluation, we applied\\ncheckpoint averaging over 10 checkpoints with 5 K iteration intervals. We sampled the audio at a\\ntemperature of 1.0.\\nTable 2 shows an objective performance measure of log-likelihood (LL) on the test set as well as\\na subjective and relative audio quality evaluation with a \\ufb01ve-scale mean opinion score (MOS) on\\n5\\nTable 3: LL ratio results with varying amount of autoregressive structural bias on the number of\\ngroups. Lower values indicate higher similarity in probability density modeling performance between\\nthe two models.\\nNumber of Groups (G)\\n4\\n8\\n16\\n32\\n64\\nLL\\nWaveFlow (5.96 M)\\n4.9785\\n5.0564\\n5.1241\\n5.141\\n5.1586\\nNanoFlow (0.75 M)\\n4.9513\\n5.0271\\n5.0797\\n5.0927\\n5.111\\nLL ratio\\n0.0272\\n0.0293\\n0.0444\\n0.0483\\n0.0476\\nnaturalness using the Amazon Mechanical Turk. Furthermore, we provide the audio synthesis speed\\nin kilohertz using a single Nvidia V100 GPU with half-precision arithmetic.\\nThe results show that our method can synthesize waveforms with a slight quality degradation against\\nthe baseline while only using approximately 1/8 of the parameters. However, the NanoFlow-naive\\nfailed to perform competitively even against a 64-channel variant of WaveFlow. This suggests that\\nfor \\ufb02ow-based models, a strict constraint of O(1) memory requirement severely degenerates the\\nmodeling capability. NanoFlow-decomp performed slightly better than NanoFlow-naive with a\\nlikelihood score of 5.13, which was still insuf\\ufb01cient as a competitive alternative.\\nOn the contrary, NanoFlow provided signi\\ufb01cantly enhanced expressiveness, with a negligible number\\nof additional parameters from the decomposition technique with \\ufb02ow indication embedding. By\\nstacking double the steps of \\ufb02ows, we further veri\\ufb01ed that the enhanced expressiveness of the \\ufb02ows\\nwas no longer proportional to the capacity of the deep generative model. Consistent with the results\\nfrom a previous work [25], we observed that the subjective MOS scores exhibited good alignment\\nwith the objective likelihood scores.\\n4.2\\nLikelihood ratio analysis with autoregressive structural bias\\nOur reference model, WaveFlow [25], provided a uni\\ufb01ed view of the expressiveness of \\ufb02ow-based\\nmodels by incorporating a \\ufb01xed amount of autoregressive structural bias into the architecture. The\\nmodel provides a hybrid method in which the autoregressive bias is proportional to the number of\\ngroup dimensions. In this section, we provide an expanded narrative on the performance gap between\\nthe non-autoregressive and autoregressive \\ufb02ows by adjusting the amount of bias for both WaveFlow\\nand NanoFlow. We trained each model with 64 channels for 500 K iterations with a batch size of two\\nfor varying degrees of the group dimension. We used D = 128 for the NanoFlow embedding.\\nTable 3 quantitatively shows the expressiveness of autoregressive bias. As we enforce a higher\\namount of the autoregressive structure into the model, we can achieve a more expressive model under\\nthe same network capacity. However, this is at the expense of sequential inference, which has been\\nreported in previous studies [19,22,29].\\nIn addition, we provide the LL ratio between WaveFlow and NanoFlow, where we measure the\\ngap in modeling capability by introducing a shared neural density estimator. Most importantly, we\\nobserved a nearly monotonic decrease in the performance gap of NanoFlow as we decreased the\\nnumber of groups. This further provides an insight into our effectiveness in utilizing the capacity\\nof the deep generative network. If we impose a lower amount of the explicit dependency between\\npartitions of data, we can extract a deep shared latent representation that is easier to manipulate by\\nour \\ufb02ow indication embedding. In other words, we can expect a wide range of \\ufb02ow-based models\\nwith bipartite coupling to bene\\ufb01t signi\\ufb01cantly from the parameterization scheme of NanoFlow.\\n4.3\\nImage density modeling results\\nTo demonstrate that our method is applicable to any con\\ufb01guration of NF and data domains, we\\nassessed the effectiveness of NanoFlow\\u2019s parameterization scheme to Glow [19]. We used the training\\ncon\\ufb01gurations of an open-source implementation as described in [19]. We trained Glow, NanoFlow,\\nand its ablations on the CIFAR10 dataset for 3,000 epochs, where all model con\\ufb01gurations reached\\nsaturation in performance. We used 256 channels and a batch size of 64 for all con\\ufb01gurations for an\\nextensive ablation study under a \\ufb01xed computational budget.\\n6\\nTable 4: Unconditional image density estimation results with bits per dimension (bpd) on CIFAR10\\nunder uniform dequantization. Results with \\u2020 were taken from the existing literature [9].\\nMethod\\nParameters (M)\\nbpd\\nGlow (256 channels)\\n15.973\\n3.40\\nGlow (512 channels) [19] \\u2020\\n44.235\\n3.35\\nGlow-large\\n287.489\\n3.30\\nRQ-NSF (C) [7] \\u2020\\n11.8\\n3.38\\nFFJORD [11] \\u2020\\n1.359\\n3.40\\nMintNet [28] \\u2020\\n27.461\\n3.32\\nFlow++ [12] \\u2020\\n32.3\\n3.28\\nResidualFlow [2] \\u2020\\n25.174\\n3.28\\nNanoFlow-naive\\n9.263\\n3.40\\nNanoFlow-decomp\\n9.935\\n3.32\\nNanoFlow\\n10.113\\n3.27\\nNanoFlow (K=48)\\n10.718\\n3.25\\nBecause NanoFlow is designed to leverage the shared density estimator with suf\\ufb01cient capacity, we\\nincreased the number of convolutional layers to six, and modi\\ufb01ed the kernel size to 3 \\u00d7 3 for all\\nlayers. We changed the kernel size of the separate density projection layers to 1 \\u00d7 1 to maintain the\\nnearly constant memory footprint of NanoFlow. We refer to the model with this modi\\ufb01ed architecture\\nwithout the application of our method as Glow-large. This model serves as an upper bound on\\nmodeling performance, but the parameter complexity is increased. We trained the original model with\\nthe exact network topology from [19] together with Glow-large to completely assess the capability of\\nNanoFlow. Because Glow uses a multi-scale architecture [6], NanoFlow is applied by sharing the\\nestimator separately for each scale. We used concatenative embedding together with multiplicative\\ngating as the \\ufb02ow indication embedding. For ek \\u2208RD, we used D = 64 for the default 32 \\ufb02ows per\\nscale, and D = 192 for a scaled-up model with 48 \\ufb02ows per scale.\\nAs presented in Table 4, we observed that the reference Glow model scaled with a higher network\\ncapacity, at the cost of the increased parameters and decreased return. NanoFlow-naive failed to\\nperform competitively, even with the increased capacity of the shared estimator. This suggests that\\neven if a more powerful neural network is introduced, a critical bottleneck exists when modeling\\nmultiple \\ufb02ows from the single model without applying our method.\\nUnlike the waveform synthesis results, applying only the decomposition technique was suf\\ufb01cient to\\noutperform NanoFlow-naive by a large margin. The performance was further improved using \\ufb02ow\\nindication embedding. NanoFlow with the default number of \\ufb02ows (32 steps per scale) exhibited\\nbetter performance than Glow-large, which has more than 28 times more parameters. This illustrates\\nthat in NFs, leveraging the shared neural network would be easier to train and more scalable with\\nbetter inductive bias, provided with proper methods as shown by NanoFlow, than employing separate\\nestimators where each neural network should learn the intermediate probability densities from scratch.\\nWhen we scaled up the model to 48 \\ufb02ows per scale, we observed an additional gain in performance\\nfrom the shared estimator, further con\\ufb01rming the scalability of the proposed method. NanoFlow\\nwas able to achieve competitive performance compared to studies with more complex non-af\\ufb01ne\\ncoupling [2,7,12,28], indicating potential bene\\ufb01ts of deep and shared latent representation. Overall,\\nthe density estimation results with bits per dimension were consistent with the audio generation\\nresults. The effectiveness of our method was further highlighted in this setup with bipartite coupling,\\nwhich further con\\ufb01rms our \\ufb01ndings from the likelihood ratio analysis in the preceding section. See\\nAppendix B for the additional results and Appendix C for the sampled images from the models.\\n5\\nRelated Work\\n5.1\\nImproving coupling transformations\\nSince the introduction of NFs into neural networks [5,27], most studies have focused on composing a\\n\\ufb02exible bijection for better expressiveness [6,7,12,13,19,21,23]. Building a more complex bijection\\n7\\ncan also achieve better memory ef\\ufb01ciency by attaining the desired level of complexity under fewer\\n\\ufb02ow operations. Our study provides an orthogonal perspective on this topic with a speci\\ufb01c focus on\\nthe parameterization of a scalable NF under a speci\\ufb01ed network capacity, where we systematically\\nassess the feasibility of employing a single shared neural density estimator for multiple \\ufb02ow steps.\\nBecause our parameterization scheme is agnostic to any setup of \\ufb02ow-based models and coupling\\noperator, we can apply any off-shelf bijections into our framework, together with improved methods\\nfor training NFs [12].\\n5.2\\nParameter sharing\\nThe concept of parameter sharing has been previously studied in various domains, from the core\\nfoundation of the design principle of convolutional and recurrent neural networks to parallel sequence\\nmodels, such as the Transformer [3,20]. The most notable example is [20] in the natural language\\nprocessing domain, which demonstrated a signi\\ufb01cantly reduced memory footprint of BERT [4] using\\na cross-layer parameter sharing of the self-attention block. We investigated the effectiveness of\\nthe weight-sharing concept on different granularities for \\ufb02ow-based models. We applied parameter\\nsharing on a model level, where a shared neural density estimator was applied to multiple stages of\\nbijective transformation that performed bijective operations. Contrary to [20], our study revealed\\nthe following \\ufb01ndings: in NFs, sharing an entire block failed to competitively model the probability\\ndensity, whereas minimal relaxation from the decomposition was critical to the performance.\\nIt is noteworthy that continuous-time normalizing \\ufb02ows (CNF) [1,11] features a form of the \\\"shared\\\"\\nneural network f. The central difference between CNF and NanoFlow (and non-continuous NFs\\nin general) is that CNF formulates the transformation by an ordinary differential equation (ODE)\\nwith an iterative evaluation of f to reach an error tolerance of the adaptive ODE solver, whereas NFs\\ndirectly model pre-de\\ufb01ned steps of transformation with fk (or f in NanoFlow) with a single network\\npass. The effectiveness and potential bene\\ufb01ts of the shared f outside the ODE-based CNFs are yet to\\nbe studied in the literature, which we aim to systematically address with NanoFlow.\\n6\\nDiscussion\\nIn this study, we presented an extensive and systematic analysis of the feasibility and potential bene\\ufb01ts\\nof using a single shared neural density estimator for multiple \\ufb02ow operations. Based on the analysis,\\nwe developed a novel parameterization scheme called NanoFlow, which enabled scalable NFs with a\\nnearly constant memory complexity and competitive performance as both a generative and a density\\nestimation model, owing to the compact network capacity. This enables direct control over the\\ntradeoff between expressiveness and inference latency, which is bene\\ufb01cial in domains where compact\\nparameterization is desired. The target performance can be explicitly designed using NanoFlow\\nas a building block depending on the task requirements, which can be useful for practitioners who\\nincorporate NFs into applications.\\nThe decomposed view on building \\ufb02ow-based models with NanoFlow suggests that two directions\\ncan be endeavored in future research: composing more expressive bijections, which has been the\\nprimary focus in existing literature, and building an optimized neural density estimator that can\\npotentially provide a more adaptive computation path leveraged by \\ufb02ow indication embedding.\\nFurthermore, these proposed future studies can be expanded from [12], which investigated better\\nneural architecture designs for building \\ufb02ow-based models using self-attention for the estimator.\\nCombined with increasing evidence in other research domains applying similar architecture [20],\\nwe expect the self-attention-based estimator to provide more expressive density estimations [8,24],\\nwhere the attention mechanism could be directly augmented from \\ufb02ow indication embedding. We\\nleave this research direction for future works.\\nIn summary, NanoFlow, which is a bijection-agnostic and generalized solution that achieves signi\\ufb01cant\\nsavings in network capacity, provides an alternative method for parameterizing NFs. Extensive\\nexperiments on real-world data domains have provided deep insights into the relationship between\\nthe capacity of deep generative models and the expressiveness of \\ufb02ow operations, along with possible\\nfuture research directions. We hope that the modular scheme of NanoFlow will motivate researchers\\nto further develop \\ufb02exible and scalable \\ufb02ow-based models.\\n8\\nBroader Impact\\nThe main motivation of this study was to observe a major hurdle in incorporating a powerful\\ngenerative capability of NFs to various application domains, where we need signi\\ufb01cantly larger\\nneural network capacity to reach the desired level of performance. As our work would impact the\\npracticality of NFs as a mainstream probabilistic toolkit, practitioners should be cautious about\\npossible misrepresentations of our \\ufb02ow indication embedding methods depending on how one further\\naugments the embedding to speci\\ufb01c tasks of interest.\\nIn particular, although we demonstrated that our \\ufb02ow indication embedding is domain agnostic\\nand independent variables, it is possible to incorporate task-speci\\ufb01c priors into our framework,\\nwhich can potentially achieve better control of the latent space. By contrast, there is a risk of\\npotential misinterpretation of the embedding, together with the latent space, from biases inside the\\ndataset. Because NFs have exact latent spaces that can be useful for downstream tasks such as facial\\nmanipulation [19], it would have a higher chance of direct exposure to various levels of biases. This\\ncould result in a potential exploitation of our embedding methods as an explainable or predictive\\nembedding vector of the biased aspects that could be inherent in the data. Considering these possible\\ndirections for the downstream applications of NFs, one should be cautious about extrapolating our\\nembedding scheme in attempts to build improved embedding methods for the target tasks, particularly\\nwhen leveraging priors into the independent variables we demonstrated.\\nAcknowledgements\\nWe thank Wei Ping for helpful discussion and feedback on implementation details of WaveFlow [25]\\nmodel. We also thank Heeseung Kim for careful proofreading. This work was supported by the BK21\\nFOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National\\nUniversity in 2020 and the National Research Foundation of Korea (NRF) grant funded by the Korea\\ngovernment (Ministry of Science and ICT) [No. 2018R1A2B3001628].\\nReferences\\n[1] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\\ndifferential equations. In Advances in neural information processing systems, pages 6571\\u20136583,\\n2018.\\n[2] Tian Qi Chen, Jens Behrmann, David K Duvenaud, and J\\u00f6rn-Henrik Jacobsen. Residual \\ufb02ows\\nfor invertible generative modeling. In Advances in Neural Information Processing Systems,\\npages 9913\\u20139923, 2019.\\n[3] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \\u0141ukasz Kaiser. Uni-\\nversal transformers. arXiv preprint arXiv:1807.03819, 2018.\\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\n[5] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\\nestimation. arXiv preprint arXiv:1410.8516, 2014.\\n[6] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In\\nInternational Conference on Learning Representations, 2017.\\n[7] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline \\ufb02ows. In\\nAdvances in Neural Information Processing Systems, pages 7509\\u20137520, 2019.\\n[8] Rasool Fakoor, Pratik Chaudhari, Jonas Mueller, and Alexander J Smola. Trade: Transformers\\nfor density estimation. arXiv preprint arXiv:2004.02441, 2020.\\n[9] Chris Finlay, J\\u00f6rn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. How to train\\nyour neural ode: the world of jacobian and kinetic regularization. In International Conference\\non Machine Learning, 2020.\\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural\\ninformation processing systems, pages 2672\\u20132680, 2014.\\n9\\n[11] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:\\nFree-form continuous dynamics for scalable reversible generative models. In International\\nConference on Learning Representations, 2019.\\n[12] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving\\n\\ufb02ow-based generative models with variational dequantization and architecture design. In\\nInternational Conference on Machine Learning, pages 2722\\u20132730, 2019.\\n[13] Emiel Hoogeboom, Rianne Van Den Berg, and Max Welling. Emerging convolutions for\\ngenerative normalizing \\ufb02ows. In International Conference on Machine Learning, pages 2771\\u2013\\n2780, 2019.\\n[14] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\\n[15] Sungwon Kim, Sang-Gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet:\\nA generative \\ufb02ow for raw audio. In International Conference on Machine Learning, pages\\n3370\\u20133378, 2019.\\n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[17] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max\\nWelling. Improved variational inference with inverse autoregressive \\ufb02ow. In Advances in Neural\\nInformation Processing Systems, pages 4743\\u20134751, 2016.\\n[18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Confer-\\nence on Learning Representations, 2013.\\n[19] Durk P Kingma and Prafulla Dhariwal. Glow: Generative \\ufb02ow with invertible 1x1 convolutions.\\nIn Advances in neural information processing systems, pages 10215\\u201310224, 2018.\\n[20] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv\\npreprint arXiv:1909.11942, 2019.\\n[21] Xuezhe Ma, Xiang Kong, Shanghang Zhang, and Eduard Hovy. Macow: Masked convolutional\\ngenerative \\ufb02ow. In Advances in Neural Information Processing Systems, pages 5891\\u20135900,\\n2019.\\n[22] Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,\\nGeorge Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet:\\nFast high-\\ufb01delity speech synthesis. In International Conference on Machine Learning, pages\\n3918\\u20133926, 2018.\\n[23] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive \\ufb02ow for density\\nestimation. In Advances in Neural Information Processing Systems, pages 2338\\u20132347, 2017.\\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\\nand Dustin Tran. Image transformer. In International Conference on Machine Learning, pages\\n4055\\u20134064, 2018.\\n[25] Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song. Wave\\ufb02ow: A compact \\ufb02ow-based model\\nfor raw audio. In International Conference on Machine Learning, 2020.\\n[26] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \\ufb02ow-based generative network\\nfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages 3617\\u20133621. IEEE, 2019.\\n[27] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing \\ufb02ows. In\\nInternational Conference on Machine Learning, pages 1530\\u20131538, 2015.\\n[28] Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks\\nwith masked convolutions. In Advances in Neural Information Processing Systems, pages\\n11004\\u201311014, 2019.\\n[29] A\\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\\nGraves, Nal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative\\nmodel for raw audio. In SSW, page 125, 2016.\\n[30] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\\nIn International Conference on Machine Learning, pages 1747\\u20131756, 2016.\\n10\\n\",\n          \"A four neuron circuit accounts for change sensitive \\ninhibition in salamander retina \\nJeffrey L. Teeters \\nLawrence Livennore Lab \\nPO Box 808, L-426 \\nLivennore CA 94550 \\nFrank H. Eeckman \\nLawrence Livennore Lab \\nPO Box 808, L-270 \\nLivennore CA 94550 \\nFrank S. Werblin \\nUC-Berkeley \\nRoom 145, LSA \\nBerkeley CA 94720 \\nAbstract \\nIn salamander retina, the response of On-Off ganglion cells to a central \\nflash is reduced by movement in the receptive field surround. Through \\ncomputer simulation of a 2-D model which takes into account their \\nanatomical and physiological properties, we show that interactions \\nbetween four neuron types (two bipolar and two amacrine) may be \\nresponsible for the generation and lateral conductance of this change \\nsensitive inhibition. The model shows that the four neuron circuit can \\naccount for previously observed movement sensitive reductions in \\nganglion cell sensitivity and allows visualization and prediction of the \\nspatio-temporal pattern of activity in change sensitive retinal cells. \\n1 INTRODUCTION \\nIn the salamander retina. the response of transient (On-Off) ganglion cells to a central \\nflash is reduced by movement in the receptive field surround (Werblin. 1972; Werblin & \\nCopenhagen. 1974) as illustrated in Fig 1. This phenomenon requires the detection of \\nchange in the surround and the lateral transmission of this change sensitive inhibition to \\nthe ganglion cell dendrites. \\nWunk & Werblin (1979) showed that all ganglion cells \\nreceive change-sensitive inhibition. and Barnes & Werblin (1987) implicated a change-\\nsensitive amacrine cell with widely distributed processes. The change-sensitivity of these \\namacrine cells has been traced in part to a truncation of synaptic release from the bipolar \\ntenninals that presumably drive them (Maguire et al., 1989). The transient response of \\nthese amacrine cells, mediated by voltage gated currents (Barnes & Werblin, 1986; Eliasof \\net al., 1987) also contributes to this change sensitivity. \\nThese and other experiments suggest that interactions between four neuron types underlie \\nboth the change detection and the lateral transmission of inhibition (Werblin et al., 1988; \\nMaguire et al., 1989). To test this hypothesis and make predictions that could be \\ncompared with later experiments we have constructed a computational model of the four \\nneuron circuit and incorporated it into an overall model of the retina. This model allows \\nus to simulate the effect of inhibition generated by the four neuron circuit on ganglion \\ncells. \\n384 \\nStimulus: \\ncentral test spot \\nWindmill with 1 \\nI +1(] \\nt \\nI \\nGanglion Cell Response: \\nStationary \\nwindmill \\nSpinning \\nI---t windmill \\n1 second \\n1 \\n1 \\n~@:U( \\nResting level \\nNormaJ+ \\n------ ---------I \\nT::::t:~\\\"\\\"\\\" \\\"~\\\"] \\nFigure 1: Change-Sensitive Inhibition. Data is from Werblin (1972). \\n2 IMPLEMENTING THE HYPOTHETICAL CIRCUIT \\nThe proposed change-sensitive circuit (Werblin et al.. 1988; Maguire et al .\\u2022 1989) is \\nreproduced in Figure 2. This is meant to describe a very local region of the retina where \\nthe receptive fields of the two bipolar cells are spatially overlapping. When a visual \\ntarget enters this receptive field. the bipolar cells are both depolarized. The sustained \\nbipolar cell activates the narrow field amacrine cell that. in tum feeds back to the synaptic \\nterminal of the transient bipolar cell to truncate transmitter release after a brief (ca. 100 \\nmsec) delay. Because the signal reaching the wide field amacrine cell is truncated after \\nabout 100 msec. the wide field amacrine cell will receive excitation when the target enters \\nthe recepti ve field. but will not continue to respond in the presence of the target. \\nThe spatial profiles of synaptic input and output for the cell types involved in the model \\nare summarized in Figure 3. The bipolar and narrow field amacrine cell sensitivities \\nextend over a region corresponding roughly to their dendritic spread. The wide field \\namacrine cell appears to receive input over a local region near the cell body, but delivers \\nits inhibitory output over a much wider region corresponding the the full extent (ca. 500 \\nmm) of its processes. \\nFigure 4 shows the electrical circuit model for each cell type. and illustrates the \\ninteractions between cells that are implemented in the model. In Figure 4. boxes contain \\nthe circuit for each cell and arrows between them represent synaptic interactions thought \\n.\\u00b7\\u00b7\\u00b7\\u00b7NarrowJietd (amacrine .... \\nTo \\n_ Ganglion \\ncells. \\nFigure 2: Circuitry to be Analyzed \\n500 \\nWide \\nBipolar 1\\\\ (Inpul and oulpull \\nNarrow fleld~ (Input and output) \\namacrine / \\n\\\" \\nfield \\n~Inpull \\nDistance from 0 cell center (Ilm) \\nFigure 3: Spatial Profiles of Input Sensitivity and Output Transmission \\n500 \\nto occur as determined through experiments in which a neurotransmitter is puffed onto \\nbipolar dendrites. Bipolar cells are modeled using two compartments. corresponding to \\nthe cell body and axon terminal as suggested in Maguire et at. (1989). Amacrine cells are \\nmodeled using only one compartment as in Eliasof et at. (1987). \\nEach compartment has a voltage (Vbs. Vbst, Vbtt. Van. Vaw). The cell body for the \\nsustained and transient bipolar are assumed to be the same. Batteries in the figure \\ncorrespond to excitatory (E+. Ena) or inhibitory reversal potentials (E-. Ek, Eel). \\nResistors represent ionic conductances. Circles and arrows through resisters indicate \\ntransmitter dependent conductances which are controlled by the voltage of a presynaptic or \\nsame cell. Functions relating voltages to conductances are mostly linear with a threshold. \\nMore details are given in Teeters et at. (1991). \\nNeurotransmitter Input \\nWide field \\nFi~ure 4: Details of Circuitry \\nA Four Neuron Circuit Accounts for Change Sensitive Inhibition \\n387 \\n3 TESTING THE COMPUTATIONAL MODEL \\nComputer simulation was used to tune model parameters. and test whether the single cell \\nproperties and proposed interactions between cells shown in Figure 4 are consistent with \\nthe responses recorded from the neurons during applications of a neurotransmitter puff. \\nResults are shown in Figure 5. Voltage clamp experiments electrically clamp the cell \\nmembrane potential to a constant voltage and determine the current required to maintain \\nthe voltage over time. Downward traces indicate that current is flowing into the cell; \\nupward traces indicate outward current For simplicity. scales are not shown, but in all \\ncases the magnitude of the simulated response is close to that of the observed response. \\nThe simulated and observed responses voltage clamps of the wide field amacrine shown in \\nthe fourth row vary because there is a sustained outward current observed experimentally \\nthat is not apparent in the simulations. This shows that the model is not perfect and is \\nsomething that needs further investigation. \\nThis difference between the model and observed response does not prevent the \\nhypothesized function of the circuit from being simulated. This is shown on the bottom \\nrow where both the observed and simulated voltage responses from the wide field amacrine \\nare transient. \\n4 SIMULATING INHIBITION TO GANGLION CELLS \\nFigure 5 illustrates that we have, to a large degree, succeeded in combining the \\ncharacteristics of single cells into a model which can explain many of the observed \\nproperties thought to be due to the interaction between these cells in a local region. \\nExperiment \\nObserved response \\nSimulated Response \\nNeurotransm Itter \\nJ \\n-.r--\\n-\\nPuff Input \\nVoltage clamp of \\n\\\"-\\nbipOlar cell body \\nVoltage clamp of V' \\nE=======:: \\nnarrow field amacrine \\n-\\nWide field amacrine \\n~., \\n--v-\\nVoltage clamp \\n-\\\" \\ny-\\nVoltage clamp with ~ \\n-V-\\npIcrotoxin block \\nVoltage response P--\\n--\\\"'-\\nFigure 5: Example Puff Simulations \\n388 \\nTeeters, Eeckman, and Werblin \\nThe next step in our analysis is to investigate how this circuit influences the response of \\nganglion cells. To do this requires simulating the input to the bipolar dendrites and \\nsimulating the ganglion cells which receive the transient inhibition generated by the wide \\nfield amacrine. This amounts to a integrated model of an entire patch of retina. including \\nreceptors. horizontal cells. the four neuron circuit discussed earlier. and ganglion cells. \\nThe manner in which we accomplish this is illustrated in Figure 6. \\nThe left side of figure 6 shows the model elements. Receptors and horizontal cells are \\nmodeled as low pass filters with different time constants and different spatial inputs. The \\nganglion cell model receives a transient excitatory input generated phenomenologically by \\na thresholded high pass filter from the transient bipolar. Inhibitory input to the ganglion \\ncell is implemented as coming from the transient wide field amacrine cells described \\npreviously. For simplicity. voltage gated currents and spiking are not implemented in the \\nganglion cell model. and only the off bipolar pathways are simulated. \\nThe right hand of Figure 6 illustrates how the model is implemented spatially. The \\ncircuit for each cell type is duplicated across the retina patch in a matrix fonnat. The \\nknown spatial properties of each cell. such as the spatial range of transmitter sensitivity \\nand release are incorporated into the model. Details are given in Teeters et al. 1991. \\n5 SIMULATING INHIBITION TO GANGLION CELLS \\nTo test if the model can account for the observed reduction in ganglion cell response \\nduring movement in the receptive field surround. we simulated the experiment depicted in \\nFigure 1. mainly the flashing of a central light during the presence of a stationary and \\nspinning windmill. The results are shown in Figure 7. \\nModel Elements \\nReceptor \\nR \\u00b7\\\" \\nHorizontal Cell \\nThreshold \\nHigh-pass \\nfilter \\n... \\nang Ion \\nel \\nwCf). 1: 'b~ .... \\n. . \\n. \\n\\u00b7 \\n... 1--\\n~E -\\n.'. \\n-\\nE\\n'\\u00b7\\u00b7 \\n\\u00b7t:t- -rEcIT r } \\n............. . \\nSpatial Implementation \\n\\u2022 \\n\\u2022 \\nOn-Off Ganglion cells \\nFigure 6: Integrated Retinal Model \\nA Four Neuron Circuit Accounts for Change Sensitive Inhibition \\n389 \\nRather than displaying a single curve representing the response of a single unit over time, \\nFigure 7 shows the simultaneous pattern of activity in an array of neurons spatially \\ndistributed across the retina patch at an instant in time (just after a central light spot is \\nturned on). The neuron responses are the transient bipolar terminal, the wide field \\namacrine neurotransmitter release, and the ganglion cell voltage response. On the left \\ncolumn is shown the response to a flashing spot when the windmill is stationary. On the \\nright is shown the response to the same flashing spot but with a spinning windmill. \\nWhen the windmill is stationary, the transient bipolar terminal responds only to the \\ncenter flash. Responses to the windmill vanes are suppressed by the narrow field \\namacrine cell causing the appearance of four regions of hyperpolarizing responses around \\nthe center. The wide field amacrine responds to the central test flash and releases \\ntransmitter as shown in the second row. The array of ganglion cells responds to both the \\nexcitatory input generated by the spot at the bipolar terminals and the inhibitory input \\ngenerated by the wide field amacrines. Because the wide field inhibition has not yet taken \\neffect at this point in time, the ganglion cells respond well to the flashing spot. \\nWhen the windmill is spinning, as is shown on the right hand column, the transient \\nbipolar terminals generate a response to the leading edge of the windmill vanes. The wide \\nfield amacrine cells receive excitatory input from the transient bipolar terminal responses \\nto the vane, and consequently release inhibitory neurotransmitter over a wide area as \\nshown in in the right column. Because inhibition is being continuously generated by the \\nspinning windmill, the response of the ganglion cells across the retinal patch has a large \\nStationary Windmill \\nSpinning windmill \\nTransient Bipolar Terminal \\nWide field \\nGanglion cell \\nFig. 7 - Ganglion Cell Inhibition Caused By Spinning Windmill \\nbowl shaped area of hyperpolarization which reduces the ganglion cell response of the \\ncells to the central test flash. This is seen by the fact that the height of depolarization in \\nthe centrally located ganglion cells is much smaller under conditions of a spinning \\nwindmill than if the windmill is stationary. This is consistent with the results found \\nexperimentally which are illustrated in Figure 1. Experimental data not yet attained. but \\nwhich are predicted by the model simulations illustrated in Figure 7. are the spatial \\npatterns of activity generated in the bipolar. amacrine. and ganglion cells in response to \\nthe different stimuli. \\n6 SUMMARY \\nUsing computer simulation of a neurophysiologically based model. we demonstrate that \\nthe experimental data describing properties of four neurons in the inner retina are \\ncompatible with the hypothesis that these neurons are involved in the detection of change \\nand the feedforward of change-sensitive inhibition to ganglion cells. First. we build a \\ncomputational model of the hypothesized four neuron circuit and determine that the \\nproposed interactions between them are sufficient to reproduce many of the observed \\nnetwork properties in response to a puff of neurotransmitter. Next. we integrate this \\nmodel into a full retina model to simulate their influence on ganglion cell responses. \\nThe model verifies the consistency of presently available data. and allows formation of \\npredictions of neural activity are subject to refutation or verification by new experiments. \\nWe are currently recording the spatio-temporal response of ganglion cells to moving \\nstimuli so that direct comparisons to these model predictions can be made. \\nReferences \\nBarnes. S. and Werblin. F.S. (1986). Gated currents generate single spike activity in \\namacrine cells of the tiger salamander. Proc. Natl. Acad. Sci. USA 83: 1509 - 1512. \\nBarnes. S. and Werblin. F.S. (1987). Direct excitatory and lateral inhibitory synaptic \\ninputs to amacrine cells in the tiger salamander retina. Brain Res. 406: 233 - 237. \\nEliasof S .\\u2022 Barnes S. and Werblin. F.S. (1987). The interaction of ionic currents \\nmediating single spike activity in retinal amacrine cells of the tiger salamander. 1. \\nNeurosci. 7: 3512 - 3524. \\nMaguire. G .\\u2022 Lukasiewicz. P. and Werblin F.S. (1989). Amacrine cell interactions under-\\nlying the response to change in the tiger salamander retina. 1. Neurosci. 9: 726 - 735. \\nTeeters. J.L .\\u2022 Eeckman. F.H .\\u2022 Werblin F.S. (1991). A computer model to visualize \\nchange sensitive responses in the salamander retina. In MA. Arbib and J-P. Ewert (eds.) \\nVisuomotor Coordination: Amphibians. Comparisons. Models and Robots. Plenum. \\nWerblin. F.S. (1972). Lateral interactions at inner plexiform layer of a vertebrate retina: \\nantagonistic response to change. Science. 175: 1008 - 1010. \\nWerblin. F.S. and Copenhagen. D.R. (1974). Control of retinal sensitivity. III. Lateral \\ninteractions at the inner plexiform layer. 1. Gen. Physiol. 63: 88 - 110. \\nWerblin. F.S .\\u2022 Maguire. G., Lukasiewicz, P., Eliasof. S .\\u2022 and Wu. S. (1988). Neural \\ninteractions mediating the detection of motion in the retina of the tiger salamander. Visual \\nNeurosci. 1: 317 - 329. \\nWunk, D.F. and Werblin, F.S. (1979). Synaptic inputs to ganglion cells in the tiger \\nsalamander retina. 1. Gen. Physiol. 73: 265 - 286. \\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df.loc[0,'pdf_text']\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ixa_8z4qKMZA",
        "outputId": "bfcad84d-05e9-422d-e8c5-be6d7f52a9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Real Time Image Saliency for Black Box Classiﬁers\\nPiotr Dabkowski\\npd437@cam.ac.uk\\nUniversity of Cambridge\\nYarin Gal\\nyarin.gal@eng.cam.ac.uk\\nUniversity of Cambridge\\nand Alan Turing Institute, London\\nAbstract\\nIn this work we develop a fast saliency detection method that can be applied to\\nany differentiable image classiﬁer. We train a masking model to manipulate the\\nscores of the classiﬁer by masking salient parts of the input image. Our model\\ngeneralises well to unseen images and requires a single forward pass to perform\\nsaliency detection, therefore suitable for use in real-time systems. We test our\\napproach on CIFAR-10 and ImageNet datasets and show that the produced saliency\\nmaps are easily interpretable, sharp, and free of artifacts. We suggest a new metric\\nfor saliency and test our method on the ImageNet object localisation task. We\\nachieve results outperforming other weakly supervised methods.\\n1\\nIntroduction\\nCurrent state of the art image classiﬁers rival human performance on image classiﬁcation tasks,\\nbut often exhibit unexpected and unintuitive behaviour [6, 13]. For example, we can apply a small\\nperturbation to the input image, unnoticeable to the human eye, to fool a classiﬁer completely [13].\\nAnother example of an unexpected behaviour is when a classiﬁer fails to understand a given class\\ndespite having high accuracy. For example, if “polar bear” is the only class in the dataset that contains\\nsnow, a classiﬁer may be able to get a 100% accuracy on this class by simply detecting the presence\\nof snow and ignoring the bear completely [6]. Therefore, even with perfect accuracy, we cannot\\nbe sure whether our model actually detects polar bears or just snow. One way to decouple the two\\nwould be to ﬁnd snow-only or polar-bear-only images and evaluate the model’s performance on these\\nimages separately. An alternative is to use an image of a polar bear with snow from the dataset and\\napply a saliency detection method to test what the classiﬁer is really looking at [6, 11].\\nSaliency detection methods show which parts of a given image are the most relevant to the model\\nfor a particular input class. Such saliency maps can be obtained for example by ﬁnding the smallest\\nregion whose removal causes the classiﬁcation score to drop signiﬁcantly. This is because we expect\\nthe removal of a patch which is not useful for the model not to affect the classiﬁcation score much.\\nFinding such a salient region can be done iteratively, but this usually requires hundreds of iterations\\nand is therefore a time-consuming process.\\nIn this paper we lay the groundwork for a new class of fast and accurate model-based saliency\\ndetectors, giving high pixel accuracy and sharp saliency maps (an example is given in ﬁgure 1). We\\npropose a fast, model agnostic, saliency detection method. Instead of iteratively obtaining saliency\\nmaps for each input image separately, we train a model to predict such a map for any input image in a\\nsingle feed-forward pass. We show that this approach is not only orders-of-magnitude faster than\\niterative methods, but it also produces higher quality saliency masks and achieves better localisation\\nresults. We assess this with standard saliency benchmarks and introduce a new saliency measure.\\nOur proposed model is able to produce real-time saliency maps, enabling new applications such as\\nvideo-saliency which we comment on in our Future Research section.\\n2\\nRelated work\\nSince the rise of CNNs in 2012 [5] numerous methods of image saliency detection have been proposed.\\nOne of the earliest such methods is a gradient-based approach introduced in [11] which computes\\nthe gradient of the class with respect to the image and assumes that salient regions are at locations\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n(a) Input Image\\n(b) Generated saliency map\\n(c) Image multiplied by the mask\\n(d) Image multiplied by inverted mask\\nFigure 1: An example of explanations produced by our model. The top row shows the explanation for the\\n\"Egyptian cat\" while the bottom row shows the explanation for the \"Beagle\". Note that produced explanations\\ncan precisely both highlight and remove the selected object from the image.\\nwith high gradient magnitude. Other similar backpropagation-based approaches have been proposed,\\nfor example Guided Backpropagation [12] or Excitation Backprop [16]. While the gradient-based\\nmethods are fast enough to be applied in real-time, they produce explanations of limited quality [16]\\nand they are hard to improve and build upon.\\nZhou et al. [17] proposed an approach that iteratively removes patches of the input image (by setting\\nthem to the mean colour) such that the class score is preserved. After a sufﬁcient number of iterations,\\nwe are left with salient parts of the original image. The maps produced by this method are easily\\ninterpretable, but unfortunately, the iterative process is very time consuming and not acceptable for\\nreal-time saliency detection.\\nIn another work, Cao et al. [1] introduced an optimisation method that aims to preserve only a fraction\\nof network activations such that the class score is maximised. Again, after the iterative optimisation\\nprocess, only activations that are relevant remain and their spatial location in the CNN feature map\\nindicate salient image regions.\\nVery recently (and in parallel to this work), another optimisation based method was proposed [2].\\nSimilarly to Cao et al. [1], Fong and Vedaldi [2] also propose to use gradient descent to optimise for\\nthe salient region, but the optimisation is done only in the image space and the classiﬁer model is\\ntreated as a black box. Essentially Fong and Vedaldi [2]’s method tries to remove as little from the\\nimage as possible, and at the same time to reduce the class score as much as possible. A removed\\nregion is then a minimally salient part of the image. This approach is model agnostic and the produced\\nmaps are easily interpretable because the optimisation is done in the image space and the model is\\ntreated as a black box.\\nWe next argue what conditions a good saliency model should satisfy, and propose a new metric for\\nsaliency.\\n3\\nImage Saliency and Introduced Evidence\\nImage saliency is relatively hard to deﬁne and there is no single obvious metric that could measure\\nthe quality of the produced map. In simple terms, the saliency map is deﬁned as a summarised\\nexplanation of where the classiﬁer “looks” to make its prediction.\\nThere are two slightly more formal deﬁnitions of saliency that we can use:\\n• Smallest sufﬁcient region (SSR) — smallest region of the image that alone allows a conﬁdent\\nclassiﬁcation,\\n2\\n• Smallest destroying region (SDR) — smallest region of the image that when removed,\\nprevents a conﬁdent classiﬁcation.\\nSimilar concepts were suggested in [2]. An example of SSR and SDR is shown in ﬁgure 2. It can\\nbe seen that SSR is very small and has only one seal visible. Given this SSR, even a human would\\nﬁnd it difﬁcult to recognise the preserved image. Nevertheless, it contains some characteristic for\\n“seal” features such as parts of the face with whiskers, and the classiﬁer is over 90% conﬁdent that\\nthis image should be labeled as a “seal”. On the other hand, SDR has a much stronger and larger\\nregion and quite successfully removes all the evidence for seals from the image. In order to be as\\ninformative as possible, we would like to ﬁnd a region that performs well as both SSR and SDR.\\nFigure 2: From left to right: the input image; smallest sufﬁcient region (SSR); smallest destroying region (SDR).\\nRegions were found using the mask optimisation procedure from [2].\\nBoth SDR and SSR remove some evidence from the image. There are few ways of removing evidence,\\nfor example by blurring the evidence, setting it to a constant colour, adding noise, or by completely\\ncropping out the unwanted parts. Unfortunately, each one of these methods introduces new evidence\\nthat can be used by the classiﬁer as a side effect. For example, if we remove a part of the image by\\nsetting it to the constant colour green then we may also unintentionally provide evidence for “grass”\\nwhich in turn may increase the probability of classes appearing often with grass (such as “giraffe”).\\nWe discuss this problem and ways of minimising introduced evidence next.\\n3.1\\nFighting the Introduced Evidence\\nAs mentioned in the previous section, by manipulating the image we always introduce some extra\\nevidence. Here, let us focus on the case of applying a mask M to the image X to obtain the edited\\nimage E. In the simplest case we can simply multiply X and M element-wise:\\nE = X ⊙M\\n(1)\\nThis operation sets certain regions of the image to a constant “0” colour. While setting a larger patch\\nof the image to “0” may sound rather harmless (perhaps following the assumption that the mean of\\nall colors carries very little evidence), we may encounter problems when the mask M is not smooth.\\nThe mask M, in the worst case, can be used to introduce a large amount of additional evidence by\\ngenerating adversarial artifacts (a similar observation was made in [2]). An example of such a mask\\nis presented in ﬁgure 3. Adversarial artifacts generated by the mask are very small in magnitude and\\nalmost imperceivable for humans, but they are able to completely destroy the original prediction of\\nthe classiﬁer. Such adversarial masks provide very poor saliency explanations and therefore should\\nbe avoided.\\nFigure 3: The adversarial mask introduces very small perturbations, but can completely alter the classiﬁer’s\\npredictions. From left to right: an image which is correctly recognised by the classiﬁer with a high conﬁdence as\\na \"tabby cat\"; a generated adversarial mask; an original image after application of the mask that is no longer\\nrecognised as a \"tabby cat\".\\n3\\nThere are a few ways to make the introduction of artifacts harder. For example, we may change the\\nway we apply a mask to reduce the amount of unwanted evidence due to speciﬁcally-crafted masks:\\nE = X ⊙M + A ⊙(1 −M)\\n(2)\\nwhere A is an alternative image. A can be chosen to be for example a highly blurred version of X.\\nIn such case mask M simply selectively adds blur to the image X and therefore it is much harder\\nto generate high-frequency-high-evidence artifacts. Unfortunately, applying blur does not eliminate\\nexisting evidence very well, especially in the case of images with low spatial frequencies like a\\nseashore or mountains.\\nAnother reasonable choice of A is a random constant colour combined with high-frequency noise.\\nThis makes the resulting image E more unpredictable at regions where M is low and therefore it is\\nslightly harder to produce a reliable artifact.\\nEven with all these measures, adversarial artifacts may still occur and therefore it is necessary to\\nencourage smoothness of the mask M for example via a total variation (TV) penalty. We can also\\ndirectly resize smaller masks to the required size as resizing can be seen as a smoothness mechanism.\\n3.2\\nA New Saliency Metric\\nA standard metric to evaluate the quality of saliency maps is the localisation accuracy of the saliency\\nmap. However, it should be noted that saliency is not equivalent to localisation. For example, in order\\nto recognise a dog we usually just need to see its head; legs and body are mostly irrelevant for the\\nrecognition process. Therefore, saliency map for a dog will usually only include its head while the\\nlocalisation box always includes a whole dog with not-salient details like legs and tail. The saliency\\nof the object highly overlaps with its localisation and therefore localisation accuracy still serves as a\\nuseful metric, but in order to better assess the quality and interpretability of the produced saliency\\nmaps, we introduce a new, highly tuned metric.\\nAccording to the SSR objective, we require that the classiﬁer is able to still recognise the object from\\nthe produced saliency map and that the preserved region is as small as possible. In order to make sure\\nthat the preserved region is free from adversarial artifacts, instead of masking we can crop the image.\\nWe propose to ﬁnd the tightest rectangular crop that contains the entire salient region and to feed that\\nrectangular region to the classiﬁer to directly verify whether it is able to recognise the requested class.\\nWe deﬁne our saliency metric simply as:\\ns(a, p) = log(˜a) −log(p)\\n(3)\\nwith ˜a = max(a, 0.05). Here a is the area of the rectangular crop as a fraction of the total image size\\nand p is the probability of the requested class returned by the classiﬁer based on the cropped region.\\nThe metric is almost a direct translation of the SSR. We threshold the area at 0.05 in order to prevent\\ninstabilities at low area fractions. Good saliency detectors will be able to signiﬁcantly reduce the\\ncrop size without reducing the classiﬁcation probability, and therefore a low value for the saliency\\nmetric is a characteristic of good saliency detectors.\\nInterpreting this metric following information theory, this measure can be seen as the relative amount\\nof information between an indicator variable with probability p and an indicator variable with\\nprobability a — or the concentration of information in the cropped region.\\nBecause most image classiﬁers accept only images of a ﬁxed size and the crop can have an arbitrary\\nsize, we resize the crop to the required size disregarding aspect ratio. This seems to work well in\\npractice, but it should be noted that the proposed saliency metric works best with classiﬁers that are\\nlargely invariant to the scale and aspect ratio of the object.\\n3.3\\nThe Saliency Objective\\nTaking the previous conditions into consideration, we want to ﬁnd a mask M that is smooth and\\nperforms well at both SSR and SDR; examples of such masks can be seen in ﬁgure 1. Therefore,\\nmore formally, given class c of interest, and an input image X, to ﬁnd a saliency map M for class c,\\nour objective function L is given by:\\nL(M) = λ1TV(M) + λ2AV(M) −log(fc(Φ(X, M))) + λ3fc(Φ(X, 1 −M))λ4\\n(4)\\nwhere fc is a softmax probability of the class c of the black box image classiﬁer and TV(M) is the\\ntotal variation of the mask deﬁned simply as:\\nTV(M) =\\nX\\ni,j\\n(Mij −Mij+1)2 +\\nX\\ni,j\\n(Mij −Mi+1j)2,\\n(5)\\n4\\nAV(M) is the average of the mask elements, taking value between 0 and 1, and λi are regularisers.\\nFinally, the function Φ removes the evidence from the image as introduced in the previous section:\\nΦ(X, M) = X ⊙M + A ⊙(1 −M).\\n(6)\\nIn total, the objective function is composed of 4 terms. The ﬁrst term enforces mask smoothness,\\nthe second term encourages that the region is small. The third term makes sure that the classiﬁer is\\nable to recognise the selected class from the preserved region. Finally, the last term ensures that the\\nprobability of the selected class, after the salient region is removed, is low (note that the inverted\\nmask 1 −M is applied). Setting λ4 to a value smaller than 1 (e.g. 0.2) helps reduce this probability\\nto very small values.\\n4\\nMasking Model\\nThe mask can be found iteratively for a given image-class pair by directly optimising the objective\\nfunction from equation 4. In fact, this is the method used by [2] which was developed in parallel to\\nthis work, with the only difference that [2] only optimises the mask iteratively and for SDR (so they\\ndon’t include the third term of our objective function). Unfortunately, iteratively ﬁnding the mask is\\nnot only very slow, as normally more than 100 iterations are required, but it also causes the mask to\\ngreatly overﬁt to the image and a large TV penalty is needed to prevent adversarial artifacts from\\nforming. Therefore, the produced masks are blurry, imprecise, and overﬁt to the speciﬁc image rather\\nthan capturing the general behaviour of the classiﬁer (see ﬁgure 2).\\nFor the above reasons, we develop a trainable masking model that can produce the desired masks\\nin a single forward pass without direct access to the image classiﬁer after training. The masking\\nmodel receives an image and a class selector as inputs and learns to produce masks that minimise our\\nobjective function (equation 4). In order to succeed at this task, the model must learn which parts of\\nthe input image are considered salient by the black box classiﬁer. In theory, the model can still learn\\nto develop adversarial masks that perform well on the objective function, but in practice it is not an\\neasy task, because the model itself acts as some sort of a “regulariser” determining which patterns are\\nmore likely and which are less.\\nFigure 4: Architecture diagram of the masking model.\\nIn order to make our masks sharp and precise, we adopt a U-Net architecture [8] so that the masking\\nmodel can use feature maps from multiple resolutions. The architecture diagram can be seen in\\nﬁgure 4. For the encoder part of the U-Net we use ResNet-50 [3] pre-trained on ImageNet [9]. It\\nshould be noted that our U-Net is just a model that is trained to predict the saliency map for the given\\nblack-box classiﬁer. We use a pre-trained ResNet as a part of this model in order to speed up the\\ntraining, however, as we show in our CIFAR-10 experiment in section 5.3 the masking model can\\nalso be trained completely from scratch.\\nThe ResNet-50 model contains feature maps of ﬁve different scales, where each subsequent scale\\nblock downsamples the input by a factor of two. We use the ResNet’s feature map from Scale 5\\n(which corresponds to downsampling by a factor of 32) and pass it through the feature ﬁlter. The\\npurpose of the feature ﬁlter is to attenuate spatial locations which contents do not correspond to\\n5\\nthe selected class. Therefore, the feature ﬁlter performs the initial localisation, while the following\\nupsampling blocks ﬁne-tune the produced masks. The output of the feature ﬁlter Y at spatial location\\ni, j is given by:\\nYij = Xijσ(XT\\nijCs)\\n(7)\\nwhere Xij is the output of the Scale 5 block at spatial location i, j; Cs is the embedding of the\\nselected class s and σ(·) is the sigmoid nonlinearity. Class embedding C can be learned as part of the\\noverall objective.\\nThe upsampler blocks take the lower resolution feature map as input and upsample it by a factor\\nof two using transposed convolution [15], afterwards they concatenate the upsampled map with the\\ncorresponding feature map from ResNet and follow that with three bottleneck blocks [3].\\nFinally, to the output of the last upsampler block (Upsampler Scale 2) we apply 1x1 convolution to\\nproduce a feature map with just two channels — C0, C1. The mask Ms is obtained from:\\nMs =\\nabs(C0)\\nabs(C0) + abs(C1)\\n(8)\\nWe use this nonstandard nonlinearity because sigmoid and tanh nonlinearities did not optimise\\nproperly and the extra degree of freedom from two channels greatly improved training. The mask Ms\\nhas resolution four times lower than the input image and has to be upsampled by a factor of four with\\nbilinear resize to obtain the ﬁnal mask M.\\nThe complexity of the model is comparable to that of ResNet-50 and it can process more than a\\nhundred 224x224 images per second on a standard GPU (which is sufﬁcient for real-time saliency\\ndetection).\\n4.1\\nTraining process\\nWe train the masking model to directly minimise the objective function from equation 4. The weights\\nof the pre-trained ResNet encoder (red blocks in ﬁgure 4) are kept ﬁxed during the training.\\nIn order to make the training process work properly, we introduce few optimisations. First of all,\\nin the naive training process, the ground truth label would always be supplied as a class selector.\\nUnfortunately, under such setting, the model learns to completely ignore the class selector and simply\\nalways masks the dominant object in the image. The solution to this problem is to sometimes supply\\na class selector for a fake class and to apply only the area penalty term of the objective function.\\nUnder this setting, the model must pay attention to the class selector, as the only way it can reduce\\nloss in case of a fake label is by setting the mask to zero. During training, we set the probability of\\nthe fake label occurrence to 30%. One can also greatly speed up the embedding training by ensuring\\nthat the maximal value of σ(XT\\nijCs) from equation 7 is high in case of a correct label and low in case\\nof a fake label.\\nFinally, let us consider again the evidence removal function Φ(X, M). In order to prevent the model\\nfrom adapting to any single evidence removal scheme the alternative image A is randomly generated\\nevery time the function Φ is called. In 50% of cases the image A is the blurred version of X (we use\\na Gaussian blur with σ = 10 to achieve a strong blur) and in the remainder of cases, A is set to a\\nrandom colour image with the addition of a Gaussian noise. Such a random scheme greatly improves\\nthe quality of the produced masks as the model can no longer make strong assumptions about the\\nﬁnal look of the image.\\n5\\nExperiments\\nIn the ImageNet saliency detection experiment we use three different black-box classiﬁers: AlexNet\\n[5], GoogleNet [14] and ResNet-50 [3]. These models are treated as black boxes and for each one\\nwe train a separate masking model. The selected parameters of the objective function are λ1 = 10,\\nλ2 = 10−3, λ3 = 5, λ4 = 0.3. The ﬁrst upsampling block has 768 output channels and with each\\nsubsequent upsampling block we reduce the number of channels by a factor of two. We train each\\nmasking model as described in section 4.1 on 250,000 images from the ImageNet training set. During\\nthe training process, a very meaningful class embedding was learned and we include its visualisation\\nin the Appendix.\\nExample masks generated by the saliency models trained on three different black box image classiﬁers\\ncan be seen in ﬁgure 5, where the model is tasked to produce a saliency map for the ground truth\\n6\\n(a) Input Image\\n(b) Model & AlexNet\\n(c) Model & GoogleNet\\n(d) Model & ResNet-50\\n(e) Grad [11]\\n(f) Mask [2]\\nFigure 5: Saliency maps generated by different methods for the ground truth class. The ground truth classes,\\nstarting from the ﬁrst row are: Scottish terrier, chocolate syrup, standard schnauzer and sorrel. Columns b, c, d\\nshow the masks generated by our masking models, each trained on a different black box classiﬁer (from left to\\nright: AlexNet, GoogleNet, ResNet-50). Last two columns e, f show saliency maps for GoogleNet generated\\nrespectively by gradient [11] and the recently introduced iterative mask optimisation approach [2].\\nlabel. In ﬁgure 5 it can be clearly seen that the quality of masks generated by our models clearly\\noutperforms alternative approaches. The masks produced by models trained on GoogleNet and\\nResNet are sharp and precise and would produce accurate object segmentations. The saliency model\\ntrained on AlexNet produces much stronger and slightly larger saliency regions, possibly because\\nAlexNet is a less powerful model which needs more evidence for successful classiﬁcation. Additional\\nexamples can be seen in the appendix A.\\n5.1\\nWeakly supervised object localisation\\nAs discussed in section 3.2 a standard method to evaluate produced saliency maps is by object\\nlocalisation accuracy. It should be noted that our model was not provided any localisation data during\\ntraining and was trained using only image-class label pairs (weakly supervised training).\\nWe adopt the localisation accuracy evaluation protocol from [1] and provide the ground truth label\\nto the masking model. Afterwards, we threshold the produced saliency map at 0.5 and the tightest\\nbounding box that contains the whole saliency map is set as the ﬁnal localisation box. The localisation\\nbox has to have IOU greater than 0.5 with any of the ground truth bounding boxes in order to consider\\nthe localisation successful, otherwise, it is counted as an error. The calculated error rates for the three\\nmodels are presented in table 1. The lowest localisation error of 36.7% was achieved by the saliency\\nmodel trained on the ResNet-50 black box, this is a good achievement considering the fact that our\\nmethod was not given any localisation training data and that a fully supervised approach employed by\\nVGG [10] achieved only slightly lower error of 34.3%. The localisation error of the model trained on\\nGoogleNet is very similar to the one trained on ResNet. This is not surprising because both models\\nproduce very similar saliency masks (see ﬁgure 5). The AlexNet trained model, on the other hand,\\nhas a considerably higher localisation error which is probably a result of AlexNet needing larger\\nimage contexts to make a successful prediction (and therefore producing saliency masks which are\\nslightly less precise).\\nWe also compared our object localisation errors to errors achieved by other weakly supervised\\nmethods and existing saliency detection techniques. As a baseline we calculated the localisation error\\n7\\nAlexnet [5]\\nGoogleNet [14]\\nResNet-50 [3]\\nLocalisation Err (%)\\n39.8\\n36.9\\n36.7\\nTable 1: Weakly supervised bounding box localisation error on ImageNet validation set for our masking models\\ntrained with different black box classiﬁers.\\nof the centrally placed rectangle which spans half of the image area — which we name \"Center\".\\nThe results are presented in table 2. It can be seen that our model outperforms other approaches,\\nsometimes by a signiﬁcant margin. It also performs signiﬁcantly better than the baseline (centrally\\nplaced box) and iteratively optimised saliency masks. Because a big fraction of ImageNet images\\nhave a large, dominant object in the center, the localisation accuracy of the centrally placed box is\\nrelatively high and it managed to outperform two methods from the previous literature.\\nCenter\\nGrad [11]\\nGuid [12]\\nCAM [18]\\nExc [16]\\nFeed [1]\\nMask [2]\\nThis Work\\n46.3\\n41.7\\n42.0\\n48.1\\n39.0\\n38.7\\n43.1\\n36.9\\nTable 2: Localisation errors(%) on ImageNet validation set for popular weakly supervised methods. Error\\nrates were taken from [2] which recalculated originally reported results using few different mask thresholding\\ntechniques and achieved slightly lower error rates. For a fair comparison, all the methods follow the same\\nevaluation protocol of [1] and produce saliency maps for GoogleNet classiﬁer [14].\\n5.2\\nEvaluating the saliency metric\\nTo better assess the interpretability of the produced masks we calculate the saliency metric introduced\\nin section 3.2 for selected saliency methods and present the results in the table 3. We include a few\\nbaseline approaches — the \"Central box\" introduced in the previous section, and the \"Max box\"\\nwhich simply corresponds to a box spanning the whole image. We also calculate the saliency metric\\nfor the ground truth bounding boxes supplied with the data, and in case the image contains more than\\none ground truth box the saliency metric is set as the average over all the boxes.\\nTable 3 shows that our model achieves a considerably better saliency metric than other saliency\\napproaches. It also signiﬁcantly outperforms max box and center box baselines and is on par\\nwith ground truth boxes which supports the claim that the interpretability of the localisation boxes\\ngenerated by our model is similar to that of the ground truth boxes.\\nLocalisation Err (%)\\nSaliency Metric\\nGround truth boxes (baseline)\\n0.00\\n0.284\\nMax box (baseline)\\n59.7\\n1.366\\nCenter box (baseline)\\n46.3\\n0.645\\nGrad [11]\\n41.7\\n0.451\\nExc [16]\\n39.0\\n0.415\\nMasking model (this work)\\n36.9\\n0.318\\nTable 3: ImageNet localisation error and the saliency metric for GoogleNet.\\n5.3\\nDetecting saliency of CIFAR-10\\nTo verify the performance of our method on a completely different dataset we implemented our\\nsaliency detection model for the CIFAR-10 dataset [4]. Because the architecture described in section\\n4 speciﬁcally targets high-resolution images and ﬁve downsampling blocks would be too much for\\n32x32 images, we modiﬁed the architecture slightly and replaced the ResNet encoder with just 3\\ndownsampling blocks with 5 convolutional layers each. We also reduced the number of bottleneck\\nblocks in each upsampling block from 3 to 1. Unlike before, with this experiment, we did not use\\na pre-trained masking model, but instead a randomly initialised one. We used a FitNet [7] trained\\nto 92% validation accuracy as a black box classiﬁer to train the masking model. All the training\\nparameters were used following the ImageNet model.\\n8\\nFigure 6: Saliency maps generated by our model for images from CIFAR-10 validation set.\\nThe masking model was trained for 20 epochs. Saliency maps for sample images from the validation\\nset are shown in ﬁgure 6. It can be seen that the produced maps are clearly interpretable and a human\\ncould easily recognise the original objects after masking. This conﬁrms that the masking model\\nworks as expected even at low resolution and that FitNet model, used as a black box learned correct\\nrepresentations for the CIFAR-10 classes. More interestingly, this shows that the masking model does\\nnot need to rely on a pre-trained model which might inject its own biases into the generated masks.\\n6\\nConclusion and Future Research\\nIn this work, we have presented a new, fast, and accurate saliency detection method that can be\\napplied to any differentiable image classiﬁer. Our model is able to produce 100 saliency masks per\\nsecond, sufﬁcient for real-time applications. We have shown that our method outperforms other\\nweakly supervised techniques at the ImageNet localisation task. We have also developed a new\\nsaliency metric that can be used to assess the quality of explanations produced by saliency detectors.\\nUnder this new metric, the quality of explanations produced by our model outperforms other popular\\nsaliency detectors and is on par with ground truth bounding boxes.\\nThe model-based nature of our technique means that our work can be extended by improving the\\narchitecture of the masking network, or by changing the objective function to achieve any desired\\nproperties for the output mask.\\nFuture work includes modifying the approach to produce high quality, weakly supervised, image\\nsegmentations. Moreover, because our model can be run in real-time, it can be used for video\\nsaliency detection to instantly explain decisions made by black-box classiﬁers such as the ones used\\nin autonomous vehicles. Lastly, our model might have biases of its own — a fact which does not\\nseem to inﬂuence the model performance in ﬁnding biases in other black boxes according to the\\nvarious metrics we used. It would be interesting to study the biases embedded into our masking\\nmodel itself, and see how these affect the generated saliency masks.\\n9\\nReferences\\n[1] Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang Wang, Zilei Wang, Yongzhen Huang, Liang\\nWang, Chang Huang, Wei Xu, Deva Ramanan, and Thomas S. Huang. Look and think twice: Capturing\\ntop-down visual attention with feedback convolutional neural networks. pages 2956–2964, 2015. doi:\\n10.1109/ICCV.2015.338. URL http://dx.doi.org/10.1109/ICCV.2015.338.\\n[2] Ruth Fong and Andrea Vedaldi. Interpretable Explanations of Black Boxes by Meaningful Perturbation.\\narXiv preprint arXiv:1704.03296, 2017.\\n[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\\nCoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.\\n[4] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, 2009. URL\\nhttp://www.cs.toronto.edu/~{}kriz/learning-features-2009-TR.pdf.\\n[5] Alex Krizhevsky,\\nIlya Sutskever,\\nand Geoffrey E Hinton.\\nImagenet classiﬁcation with\\ndeep convolutional neural networks.\\nIn F. Pereira,\\nC. J. C. Burges,\\nL. Bottou,\\nand\\nK. Q. Weinberger,\\neditors,\\nAdvances in Neural Information Processing Systems 25,\\npages\\n1097–1105.\\nCurran\\nAssociates,\\nInc.,\\n2012.\\nURL\\nhttp://papers.nips.cc/paper/\\n4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\\n[6] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the\\npredictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining, pages 1135–1144. ACM, 2016.\\n[7] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua\\nBengio. FitNets: Hints for Thin Deep Nets. CoRR, abs/1412.6550, 2014. URL http://arxiv.org/\\nabs/1412.6550.\\n[8] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\\nimage segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv.org/abs/1505.04597.\\n[9] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\\nScale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252,\\n2015. doi: 10.1007/s11263-015-0816-y.\\n[10] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\\ntion. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.\\n[11] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising\\nimage classiﬁcation models and saliency maps. CoRR, abs/1312.6034, 2013. URL http://arxiv.org/\\nabs/1312.6034.\\n[12] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for\\nsimplicity: The all convolutional net. CoRR, abs/1412.6806, 2014. URL http://arxiv.org/abs/1412.\\n6806.\\n[13] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,\\nand Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL http:\\n//arxiv.org/abs/1312.6199.\\n[14] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Du-\\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CoRR,\\nabs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.\\n[15] Matthew D. Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. CoRR,\\nabs/1311.2901, 2013. URL http://arxiv.org/abs/1311.2901.\\n[16] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by\\nexcitation backprop. 2016. URL https://www.robots.ox.ac.uk/~vgg/rg/papers/zhang_eccv16.\\npdf.\\n[17] Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio Torralba. Object Detectors Emerge\\nin Deep Scene CNNs. CoRR, abs/1412.6856, 2014. URL http://arxiv.org/abs/1412.6856.\\n[18] Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for\\ndiscriminative localization. CoRR, abs/1512.04150, 2015. URL http://arxiv.org/abs/1512.04150.\\n10\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSvgLERTKXGK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}